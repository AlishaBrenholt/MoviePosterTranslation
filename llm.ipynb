{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26bcf04382191a30"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:35:19.743691Z",
     "start_time": "2025-03-14T06:35:19.585755Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from transformers import MarianTokenizer \n",
    "import sentencepiece as sp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove randomness and set cuda"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4a8391cac9bed6b"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 14\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:20:53.606088Z",
     "start_time": "2025-03-14T06:20:53.579562Z"
    }
   },
   "id": "a16b9fed86c9852e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7692146dc22d0a8"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "DEV_PATH = \"data/llm/development.json\"\n",
    "EVAL_PATH = \"data/llm/evaluation.json\"\n",
    "\n",
    "# English = \"data/llm/OpenSubtitles.en-ko.en\"\n",
    "# Korean = \"data/llm/OpenSubtitles.en-ko.ko\"\n",
    "# check_dash(Korean,\"ko\", \"data/llm/korean\")\n",
    "# # check_dash(English,\"en\", \"data/llm/english\")\n",
    "# def check_dash(path,language, new_path):\n",
    "#     \"\"\"This functions checks for a dash in the middle of the sentence with spaces on \n",
    "#     either side. This is a common error in the data.\n",
    "#     example: \"- 뭐요? - 멈춰\"?\n",
    "#     We want to add a newline before the dash in the middle of the sentence.\n",
    "#     \"\"\"\n",
    "#     # First create/clear the files\n",
    "#     f = open(new_path + f\".{language}\", \"w\")\n",
    "#     data = get_data(path)\n",
    "#     count = 0\n",
    "#     next_count = 1\n",
    "#     for sentence in data:\n",
    "#         if \" - \" in sentence:\n",
    "#             sentence = sentence.replace(\" - \", f\"\\n{count+1} \")\n",
    "#             next_count += 1\n",
    "#         if \"- \" in sentence:\n",
    "#             sentence = sentence.replace(\"- \", \"\")\n",
    "#         f.write(f\"{count} \" + sentence)\n",
    "#         count = next_count\n",
    "#         next_count += 1\n",
    "    \n",
    "def get_data(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def make_plain_text(data, path):\n",
    "    f = open(path + \".en\", \"w\")\n",
    "    g = open(path + \".ko\", \"w\")\n",
    "    for group in data:\n",
    "        sentences = group[\"text\"]\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence[\"en_text\"] + \"\\n\")\n",
    "            g.write(sentence[\"ko_text\"] + \"\\n\")\n",
    "    f.close()\n",
    "    g.close()\n",
    "# test = get_data(TEST_PATH)\n",
    "train_json = get_data(DEV_PATH)\n",
    "test_json = get_data(EVAL_PATH)\n",
    "make_plain_text(train_json, \"data/llm/development\")\n",
    "make_plain_text(test_json, \"data/llm/evaluation\")\n",
    "\n",
    "# bpe tokenization\n",
    "!mkdir -p models/tokens\n",
    "vocab_size = 2000\n",
    "threads = 16\n",
    "\n",
    "# this is used latter in batches, apparently it crashes if batches have different length sentences inside of themselves\n",
    "padding_id = 3 \n",
    "\n",
    "sp.SentencePieceTrainer.train(input=\"data/llm/development.en\", model_prefix=\"models/tokens/english\", vocab_size=vocab_size, model_type=\"bpe\", num_threads=threads, bos_id=0, eos_id=1, unk_id=2, pad_id=padding_id) # force the padding id because it didn't automatically make it\n",
    "sp.SentencePieceTrainer.train(input=\"data/llm/development.ko\", model_prefix=\"models/tokens/korean\", vocab_size=vocab_size, model_type=\"bpe\", num_threads=threads, bos_id=0, eos_id=1, unk_id=2, pad_id=padding_id) # force the padding id because it didn't automatically make it\n",
    "english_tokenizer = sp.SentencePieceProcessor()\n",
    "english_tokenizer.load(\"models/tokens/english.model\")\n",
    "korean_tokenizer = sp.SentencePieceProcessor()\n",
    "korean_tokenizer.load(\"models/tokens/korean.model\")\n",
    "assert(english_tokenizer.pad_id() == 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:20:54.275497Z",
     "start_time": "2025-03-14T06:20:53.584136Z"
    }
   },
   "id": "cc221a830719a842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the vocabs of each language"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "335f984bee913472"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 2000\n",
      "Korean vocab size: 2000\n",
      "reverse vocab size: 2000\n"
     ]
    }
   ],
   "source": [
    "def load_sentencepiece_vocab(model_file):\n",
    "    spp = sp.SentencePieceProcessor()\n",
    "    spp.load(model_file)\n",
    "    vocab = {}\n",
    "    reverse_vocab = {}\n",
    "    for i in range(spp.get_piece_size()):\n",
    "        vocab[spp.id_to_piece(i)] = i\n",
    "        reverse_vocab[i] = spp.id_to_piece(i)\n",
    "    return vocab, reverse_vocab\n",
    "\n",
    "# Load English and Korean vocabularies\n",
    "english_vocab, reverse_english_vocab = load_sentencepiece_vocab(\"models/tokens/english.model\")\n",
    "korean_vocab, reverse_korean_vocab = load_sentencepiece_vocab(\"models/tokens/korean.model\")\n",
    "\n",
    "print(\"English vocab size:\", len(english_vocab))\n",
    "print(\"Korean vocab size:\", len(korean_vocab))\n",
    "print(\"reverse vocab size:\", len(reverse_english_vocab))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:20:54.285733Z",
     "start_time": "2025-03-14T06:20:54.281771Z"
    }
   },
   "id": "922c4a3fb6e413c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset and DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a366a9613a590e6"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_path, korean_path, english_tokenizer, korean_tokenizer):\n",
    "        with open(english_path, \"r\") as file:\n",
    "            self.english_sentences = file.readlines()\n",
    "        with open(korean_path, \"r\") as file:\n",
    "            self.korean_sentences = file.readlines()\n",
    "        assert len(self.english_sentences) == len(self.korean_sentences)\n",
    "        self.english_tokenizer = english_tokenizer\n",
    "        self.korean_tokenizer = korean_tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        english_sentence = self.english_sentences[i]\n",
    "        korean_sentence = self.korean_sentences[i]\n",
    "        english_tokens = self.english_tokenizer.encode(english_sentence)\n",
    "        korean_tokens = self.korean_tokenizer.encode(korean_sentence)\n",
    "        return torch.tensor(english_tokens), torch.tensor(korean_tokens)\n",
    "    \n",
    "# This is the padding class which allows us to use batches with dynamic sentence lengths\n",
    "class Padding():\n",
    "    def __init__(self, padding_id):\n",
    "        self.padding_id = padding_id\n",
    "    def __call__(self, batch):\n",
    "        english, korean = zip(*batch)\n",
    "        english = pad_sequence(english, batch_first=True, padding_value=self.padding_id)\n",
    "        korean = pad_sequence(korean, batch_first=True, padding_value=self.padding_id)\n",
    "        return english, korean\n",
    "    \n",
    "padding = Padding(padding_id)\n",
    "dataset = TranslationDataset(\"data/llm/development.en\", \"data/llm/development.ko\", english_tokenizer, korean_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=padding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:20:54.323298Z",
     "start_time": "2025-03-14T06:20:54.303802Z"
    }
   },
   "id": "9da6996a99b8352b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eeed24ee190e5f8"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Note I took a lot of inspiration from https://medium.com/@WamiqRaza/sequence-to-sequence-learning-with-neural-networks-30028d824591\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "        prediction = self.linear(output)\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, english, korean):\n",
    "        hidden, cell = self.encoder(english)\n",
    "        prediction, _, _ = self.decoder(korean, hidden, cell)\n",
    "        return prediction\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "layers = 2\n",
    "encoder = Encoder(len(english_vocab), embedding_size, hidden_size, layers).to(device)\n",
    "decoder = Decoder(len(korean_vocab), embedding_size, hidden_size, layers).to(device)\n",
    "model = Model(encoder, decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:20:54.338070Z",
     "start_time": "2025-03-14T06:20:54.304068Z"
    }
   },
   "id": "7e087de471fa9f4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "939b78df88f868d9"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1/158, Loss: 7.640931606292725\n",
      "Batch: 2/158, Loss: 7.1426801681518555\n",
      "Batch: 3/158, Loss: 6.375210762023926\n",
      "Batch: 4/158, Loss: 4.94036865234375\n",
      "Batch: 5/158, Loss: 3.787696599960327\n",
      "Batch: 6/158, Loss: 5.6587748527526855\n",
      "Batch: 7/158, Loss: 4.131099700927734\n",
      "Batch: 8/158, Loss: 3.511444091796875\n",
      "Batch: 9/158, Loss: 4.775665283203125\n",
      "Batch: 10/158, Loss: 3.852273464202881\n",
      "Batch: 11/158, Loss: 3.769291639328003\n",
      "Batch: 12/158, Loss: 4.654695510864258\n",
      "Batch: 13/158, Loss: 3.414902925491333\n",
      "Batch: 14/158, Loss: 3.373291254043579\n",
      "Batch: 15/158, Loss: 4.530930519104004\n",
      "Batch: 16/158, Loss: 3.7246270179748535\n",
      "Batch: 17/158, Loss: 4.506260871887207\n",
      "Batch: 18/158, Loss: 3.6151745319366455\n",
      "Batch: 19/158, Loss: 3.9277796745300293\n",
      "Batch: 20/158, Loss: 4.056972503662109\n",
      "Batch: 21/158, Loss: 4.138716220855713\n",
      "Batch: 22/158, Loss: 3.97751784324646\n",
      "Batch: 23/158, Loss: 3.9993715286254883\n",
      "Batch: 24/158, Loss: 3.650805711746216\n",
      "Batch: 25/158, Loss: 3.268461227416992\n",
      "Batch: 26/158, Loss: 3.499732732772827\n",
      "Batch: 27/158, Loss: 3.7434568405151367\n",
      "Batch: 28/158, Loss: 3.2962915897369385\n",
      "Batch: 29/158, Loss: 3.975126266479492\n",
      "Batch: 30/158, Loss: 4.016900539398193\n",
      "Batch: 31/158, Loss: 3.991074800491333\n",
      "Batch: 32/158, Loss: 3.6310441493988037\n",
      "Batch: 33/158, Loss: 3.713376760482788\n",
      "Batch: 34/158, Loss: 4.1031670570373535\n",
      "Batch: 35/158, Loss: 3.2390334606170654\n",
      "Batch: 36/158, Loss: 3.6695139408111572\n",
      "Batch: 37/158, Loss: 3.665736675262451\n",
      "Batch: 38/158, Loss: 3.731142044067383\n",
      "Batch: 39/158, Loss: 3.3313612937927246\n",
      "Batch: 40/158, Loss: 4.210757732391357\n",
      "Batch: 41/158, Loss: 3.150167942047119\n",
      "Batch: 42/158, Loss: 3.6766319274902344\n",
      "Batch: 43/158, Loss: 3.7493178844451904\n",
      "Batch: 44/158, Loss: 3.3529603481292725\n",
      "Batch: 45/158, Loss: 3.4061357975006104\n",
      "Batch: 46/158, Loss: 3.6030948162078857\n",
      "Batch: 47/158, Loss: 3.9369635581970215\n",
      "Batch: 48/158, Loss: 2.781522750854492\n",
      "Batch: 49/158, Loss: 3.551996946334839\n",
      "Batch: 50/158, Loss: 3.275773525238037\n",
      "Batch: 51/158, Loss: 3.5633230209350586\n",
      "Batch: 52/158, Loss: 3.770822286605835\n",
      "Batch: 53/158, Loss: 3.2266123294830322\n",
      "Batch: 54/158, Loss: 3.4981350898742676\n",
      "Batch: 55/158, Loss: 3.518584728240967\n",
      "Batch: 56/158, Loss: 3.6581413745880127\n",
      "Batch: 57/158, Loss: 3.911640167236328\n",
      "Batch: 58/158, Loss: 3.391397476196289\n",
      "Batch: 59/158, Loss: 3.1171059608459473\n",
      "Batch: 60/158, Loss: 3.6126010417938232\n",
      "Batch: 61/158, Loss: 2.835538864135742\n",
      "Batch: 62/158, Loss: 3.8988633155822754\n",
      "Batch: 63/158, Loss: 2.9511630535125732\n",
      "Batch: 64/158, Loss: 3.4741854667663574\n",
      "Batch: 65/158, Loss: 2.6661717891693115\n",
      "Batch: 66/158, Loss: 3.0444610118865967\n",
      "Batch: 67/158, Loss: 2.8381779193878174\n",
      "Batch: 68/158, Loss: 3.2255194187164307\n",
      "Batch: 69/158, Loss: 3.642866373062134\n",
      "Batch: 70/158, Loss: 3.1793911457061768\n",
      "Batch: 71/158, Loss: 3.007490396499634\n",
      "Batch: 72/158, Loss: 2.442072629928589\n",
      "Batch: 73/158, Loss: 3.2055068016052246\n",
      "Batch: 74/158, Loss: 2.7536940574645996\n",
      "Batch: 75/158, Loss: 3.0120668411254883\n",
      "Batch: 76/158, Loss: 2.609560251235962\n",
      "Batch: 77/158, Loss: 2.3288421630859375\n",
      "Batch: 78/158, Loss: 2.783689498901367\n",
      "Batch: 79/158, Loss: 2.5523626804351807\n",
      "Batch: 80/158, Loss: 2.2935078144073486\n",
      "Batch: 81/158, Loss: 2.4758145809173584\n",
      "Batch: 82/158, Loss: 2.5055947303771973\n",
      "Batch: 83/158, Loss: 2.6541733741760254\n",
      "Batch: 84/158, Loss: 2.4638495445251465\n",
      "Batch: 85/158, Loss: 2.507183074951172\n",
      "Batch: 86/158, Loss: 3.0290820598602295\n",
      "Batch: 87/158, Loss: 2.977386951446533\n",
      "Batch: 88/158, Loss: 2.4096360206604004\n",
      "Batch: 89/158, Loss: 2.8381950855255127\n",
      "Batch: 90/158, Loss: 2.3935890197753906\n",
      "Batch: 91/158, Loss: 2.520573854446411\n",
      "Batch: 92/158, Loss: 2.0883398056030273\n",
      "Batch: 93/158, Loss: 2.1655778884887695\n",
      "Batch: 94/158, Loss: 2.29600191116333\n",
      "Batch: 95/158, Loss: 2.2540781497955322\n",
      "Batch: 96/158, Loss: 2.5869688987731934\n",
      "Batch: 97/158, Loss: 2.286761999130249\n",
      "Batch: 98/158, Loss: 1.7428929805755615\n",
      "Batch: 99/158, Loss: 1.7559984922409058\n",
      "Batch: 100/158, Loss: 2.5947425365448\n",
      "Batch: 101/158, Loss: 2.6445930004119873\n",
      "Batch: 102/158, Loss: 2.168912887573242\n",
      "Batch: 103/158, Loss: 1.764340877532959\n",
      "Batch: 104/158, Loss: 2.1161954402923584\n",
      "Batch: 105/158, Loss: 1.753628134727478\n",
      "Batch: 106/158, Loss: 1.675673484802246\n",
      "Batch: 107/158, Loss: 1.7607675790786743\n",
      "Batch: 108/158, Loss: 1.9943476915359497\n",
      "Batch: 109/158, Loss: 1.5594178438186646\n",
      "Batch: 110/158, Loss: 1.6775684356689453\n",
      "Batch: 111/158, Loss: 1.7662452459335327\n",
      "Batch: 112/158, Loss: 1.5024694204330444\n",
      "Batch: 113/158, Loss: 1.638330101966858\n",
      "Batch: 114/158, Loss: 1.0681588649749756\n",
      "Batch: 115/158, Loss: 1.5591905117034912\n",
      "Batch: 116/158, Loss: 1.2265254259109497\n",
      "Batch: 117/158, Loss: 1.140379786491394\n",
      "Batch: 118/158, Loss: 1.1965088844299316\n",
      "Batch: 119/158, Loss: 1.470888614654541\n",
      "Batch: 120/158, Loss: 1.4165029525756836\n",
      "Batch: 121/158, Loss: 1.3326081037521362\n",
      "Batch: 122/158, Loss: 1.0886434316635132\n",
      "Batch: 123/158, Loss: 1.3346405029296875\n",
      "Batch: 124/158, Loss: 1.2292886972427368\n",
      "Batch: 125/158, Loss: 1.2790087461471558\n",
      "Batch: 126/158, Loss: 1.1551443338394165\n",
      "Batch: 127/158, Loss: 1.2378170490264893\n",
      "Batch: 128/158, Loss: 0.9294697642326355\n",
      "Batch: 129/158, Loss: 0.8930892944335938\n",
      "Batch: 130/158, Loss: 1.1414471864700317\n",
      "Batch: 131/158, Loss: 0.894191324710846\n",
      "Batch: 132/158, Loss: 0.94606614112854\n",
      "Batch: 133/158, Loss: 0.7966756820678711\n",
      "Batch: 134/158, Loss: 0.9973163604736328\n",
      "Batch: 135/158, Loss: 0.6238375902175903\n",
      "Batch: 136/158, Loss: 0.8409538269042969\n",
      "Batch: 137/158, Loss: 0.7858228087425232\n",
      "Batch: 138/158, Loss: 0.6456507444381714\n",
      "Batch: 139/158, Loss: 0.8516089916229248\n",
      "Batch: 140/158, Loss: 0.8191525340080261\n",
      "Batch: 141/158, Loss: 0.7320737242698669\n",
      "Batch: 142/158, Loss: 0.6667499542236328\n",
      "Batch: 143/158, Loss: 0.8078839778900146\n",
      "Batch: 144/158, Loss: 0.606620192527771\n",
      "Batch: 145/158, Loss: 0.686379611492157\n",
      "Batch: 146/158, Loss: 0.6847954392433167\n",
      "Batch: 147/158, Loss: 0.667463481426239\n",
      "Batch: 148/158, Loss: 0.5135432481765747\n",
      "Batch: 149/158, Loss: 0.48672041296958923\n",
      "Batch: 150/158, Loss: 0.5483096837997437\n",
      "Batch: 151/158, Loss: 0.5172203183174133\n",
      "Batch: 152/158, Loss: 0.3875223994255066\n",
      "Batch: 153/158, Loss: 0.4684111177921295\n",
      "Batch: 154/158, Loss: 0.5833420753479004\n",
      "Batch: 155/158, Loss: 0.49423032999038696\n",
      "Batch: 156/158, Loss: 0.4821208119392395\n",
      "Batch: 157/158, Loss: 0.46006324887275696\n",
      "Batch: 158/158, Loss: 0.5881109237670898\n",
      "Epoch: 1/10, Loss: 406.5918007194996, Time: 71.71744799613953\n",
      "Batch: 1/158, Loss: 0.43565863370895386\n",
      "Batch: 2/158, Loss: 0.3601939380168915\n",
      "Batch: 3/158, Loss: 0.3753555715084076\n",
      "Batch: 4/158, Loss: 0.34972167015075684\n",
      "Batch: 5/158, Loss: 0.38120171427726746\n",
      "Batch: 6/158, Loss: 0.3198417127132416\n",
      "Batch: 7/158, Loss: 0.33439695835113525\n",
      "Batch: 8/158, Loss: 0.2891079783439636\n",
      "Batch: 9/158, Loss: 0.3433610498905182\n",
      "Batch: 10/158, Loss: 0.35930347442626953\n",
      "Batch: 11/158, Loss: 0.287617951631546\n",
      "Batch: 12/158, Loss: 0.30908986926078796\n",
      "Batch: 13/158, Loss: 0.28891515731811523\n",
      "Batch: 14/158, Loss: 0.22720961272716522\n",
      "Batch: 15/158, Loss: 0.3009571433067322\n",
      "Batch: 16/158, Loss: 0.32829150557518005\n",
      "Batch: 17/158, Loss: 0.2391837239265442\n",
      "Batch: 18/158, Loss: 0.2578243613243103\n",
      "Batch: 19/158, Loss: 0.2202572226524353\n",
      "Batch: 20/158, Loss: 0.23447605967521667\n",
      "Batch: 21/158, Loss: 0.21669134497642517\n",
      "Batch: 22/158, Loss: 0.26231592893600464\n",
      "Batch: 23/158, Loss: 0.18125338852405548\n",
      "Batch: 24/158, Loss: 0.233127161860466\n",
      "Batch: 25/158, Loss: 0.16193071007728577\n",
      "Batch: 26/158, Loss: 0.22773699462413788\n",
      "Batch: 27/158, Loss: 0.22927135229110718\n",
      "Batch: 28/158, Loss: 0.22878162562847137\n",
      "Batch: 29/158, Loss: 0.16712968051433563\n",
      "Batch: 30/158, Loss: 0.1810583472251892\n",
      "Batch: 31/158, Loss: 0.21306414902210236\n",
      "Batch: 32/158, Loss: 0.19634264707565308\n",
      "Batch: 33/158, Loss: 0.15570130944252014\n",
      "Batch: 34/158, Loss: 0.184648334980011\n",
      "Batch: 35/158, Loss: 0.1285896897315979\n",
      "Batch: 36/158, Loss: 0.19014810025691986\n",
      "Batch: 37/158, Loss: 0.16079936921596527\n",
      "Batch: 38/158, Loss: 0.1555761992931366\n",
      "Batch: 39/158, Loss: 0.13133271038532257\n",
      "Batch: 40/158, Loss: 0.17550893127918243\n",
      "Batch: 41/158, Loss: 0.17858299612998962\n",
      "Batch: 42/158, Loss: 0.14657825231552124\n",
      "Batch: 43/158, Loss: 0.1625267118215561\n",
      "Batch: 44/158, Loss: 0.15248988568782806\n",
      "Batch: 45/158, Loss: 0.12696631252765656\n",
      "Batch: 46/158, Loss: 0.1262439340353012\n",
      "Batch: 47/158, Loss: 0.18482738733291626\n",
      "Batch: 48/158, Loss: 0.1140383705496788\n",
      "Batch: 49/158, Loss: 0.12251567095518112\n",
      "Batch: 50/158, Loss: 0.1285284161567688\n",
      "Batch: 51/158, Loss: 0.10351704806089401\n",
      "Batch: 52/158, Loss: 0.10660933703184128\n",
      "Batch: 53/158, Loss: 0.11866922676563263\n",
      "Batch: 54/158, Loss: 0.13250452280044556\n",
      "Batch: 55/158, Loss: 0.10403472185134888\n",
      "Batch: 56/158, Loss: 0.09848716109991074\n",
      "Batch: 57/158, Loss: 0.07364137470722198\n",
      "Batch: 58/158, Loss: 0.08989043533802032\n",
      "Batch: 59/158, Loss: 0.08073625713586807\n",
      "Batch: 60/158, Loss: 0.10263504087924957\n",
      "Batch: 61/158, Loss: 0.09353713691234589\n",
      "Batch: 62/158, Loss: 0.10676272958517075\n",
      "Batch: 63/158, Loss: 0.08565041422843933\n",
      "Batch: 64/158, Loss: 0.10169154405593872\n",
      "Batch: 65/158, Loss: 0.09878400713205338\n",
      "Batch: 66/158, Loss: 0.08245246857404709\n",
      "Batch: 67/158, Loss: 0.07610678672790527\n",
      "Batch: 68/158, Loss: 0.08982416987419128\n",
      "Batch: 69/158, Loss: 0.09517543762922287\n",
      "Batch: 70/158, Loss: 0.11138755828142166\n",
      "Batch: 71/158, Loss: 0.09131646901369095\n",
      "Batch: 72/158, Loss: 0.07570139318704605\n",
      "Batch: 73/158, Loss: 0.09059686213731766\n",
      "Batch: 74/158, Loss: 0.0818788930773735\n",
      "Batch: 75/158, Loss: 0.08166097104549408\n",
      "Batch: 76/158, Loss: 0.09480297565460205\n",
      "Batch: 77/158, Loss: 0.07709348946809769\n",
      "Batch: 78/158, Loss: 0.10147402435541153\n",
      "Batch: 79/158, Loss: 0.0714259222149849\n",
      "Batch: 80/158, Loss: 0.08134175837039948\n",
      "Batch: 81/158, Loss: 0.07767342031002045\n",
      "Batch: 82/158, Loss: 0.07625065743923187\n",
      "Batch: 83/158, Loss: 0.08695539832115173\n",
      "Batch: 84/158, Loss: 0.09649835526943207\n",
      "Batch: 85/158, Loss: 0.07499748468399048\n",
      "Batch: 86/158, Loss: 0.05907593294978142\n",
      "Batch: 87/158, Loss: 0.05484813451766968\n",
      "Batch: 88/158, Loss: 0.06273543834686279\n",
      "Batch: 89/158, Loss: 0.08243768662214279\n",
      "Batch: 90/158, Loss: 0.07376725226640701\n",
      "Batch: 91/158, Loss: 0.07917454838752747\n",
      "Batch: 92/158, Loss: 0.06430277228355408\n",
      "Batch: 93/158, Loss: 0.06082785502076149\n",
      "Batch: 94/158, Loss: 0.09497718513011932\n",
      "Batch: 95/158, Loss: 0.04730706289410591\n",
      "Batch: 96/158, Loss: 0.09270232915878296\n",
      "Batch: 97/158, Loss: 0.05568751320242882\n",
      "Batch: 98/158, Loss: 0.067746601998806\n",
      "Batch: 99/158, Loss: 0.07104030251502991\n",
      "Batch: 100/158, Loss: 0.06500428169965744\n",
      "Batch: 101/158, Loss: 0.05258367583155632\n",
      "Batch: 102/158, Loss: 0.057517923414707184\n",
      "Batch: 103/158, Loss: 0.06477021425962448\n",
      "Batch: 104/158, Loss: 0.067708820104599\n",
      "Batch: 105/158, Loss: 0.06419957429170609\n",
      "Batch: 106/158, Loss: 0.08463630080223083\n",
      "Batch: 107/158, Loss: 0.058599960058927536\n",
      "Batch: 108/158, Loss: 0.052951812744140625\n",
      "Batch: 109/158, Loss: 0.06066964194178581\n",
      "Batch: 110/158, Loss: 0.04735030233860016\n",
      "Batch: 111/158, Loss: 0.054324183613061905\n",
      "Batch: 112/158, Loss: 0.05847582221031189\n",
      "Batch: 113/158, Loss: 0.06373371928930283\n",
      "Batch: 114/158, Loss: 0.0496317557990551\n",
      "Batch: 115/158, Loss: 0.05459604784846306\n",
      "Batch: 116/158, Loss: 0.07158416509628296\n",
      "Batch: 117/158, Loss: 0.06963637471199036\n",
      "Batch: 118/158, Loss: 0.06335314363241196\n",
      "Batch: 119/158, Loss: 0.05575656145811081\n",
      "Batch: 120/158, Loss: 0.05312758684158325\n",
      "Batch: 121/158, Loss: 0.04192306473851204\n",
      "Batch: 122/158, Loss: 0.0359845869243145\n",
      "Batch: 123/158, Loss: 0.05000679939985275\n",
      "Batch: 124/158, Loss: 0.054438453167676926\n",
      "Batch: 125/158, Loss: 0.06565272063016891\n",
      "Batch: 126/158, Loss: 0.035613641142845154\n",
      "Batch: 127/158, Loss: 0.045148029923439026\n",
      "Batch: 128/158, Loss: 0.053794339299201965\n",
      "Batch: 129/158, Loss: 0.0446898452937603\n",
      "Batch: 130/158, Loss: 0.038738589733839035\n",
      "Batch: 131/158, Loss: 0.06752324104309082\n",
      "Batch: 132/158, Loss: 0.042042121291160583\n",
      "Batch: 133/158, Loss: 0.044049423187971115\n",
      "Batch: 134/158, Loss: 0.031631264835596085\n",
      "Batch: 135/158, Loss: 0.04670552909374237\n",
      "Batch: 136/158, Loss: 0.03731565177440643\n",
      "Batch: 137/158, Loss: 0.04932718724012375\n",
      "Batch: 138/158, Loss: 0.037489935755729675\n",
      "Batch: 139/158, Loss: 0.034627385437488556\n",
      "Batch: 140/158, Loss: 0.03653395175933838\n",
      "Batch: 141/158, Loss: 0.037469811737537384\n",
      "Batch: 142/158, Loss: 0.04938369244337082\n",
      "Batch: 143/158, Loss: 0.048733849078416824\n",
      "Batch: 144/158, Loss: 0.031025508418679237\n",
      "Batch: 145/158, Loss: 0.05059811845421791\n",
      "Batch: 146/158, Loss: 0.03200151026248932\n",
      "Batch: 147/158, Loss: 0.02388625591993332\n",
      "Batch: 148/158, Loss: 0.0355432853102684\n",
      "Batch: 149/158, Loss: 0.030007950961589813\n",
      "Batch: 150/158, Loss: 0.04974006861448288\n",
      "Batch: 151/158, Loss: 0.03045019321143627\n",
      "Batch: 152/158, Loss: 0.04448230564594269\n",
      "Batch: 153/158, Loss: 0.03778518736362457\n",
      "Batch: 154/158, Loss: 0.039435550570487976\n",
      "Batch: 155/158, Loss: 0.03718113154172897\n",
      "Batch: 156/158, Loss: 0.03817690163850784\n",
      "Batch: 157/158, Loss: 0.038154538720846176\n",
      "Batch: 158/158, Loss: 0.0355326384305954\n",
      "Epoch: 2/10, Loss: 18.497703418135643, Time: 74.47580480575562\n",
      "Batch: 1/158, Loss: 0.029933206737041473\n",
      "Batch: 2/158, Loss: 0.02817394584417343\n",
      "Batch: 3/158, Loss: 0.025380373001098633\n",
      "Batch: 4/158, Loss: 0.027139529585838318\n",
      "Batch: 5/158, Loss: 0.02350759319961071\n",
      "Batch: 6/158, Loss: 0.037948887795209885\n",
      "Batch: 7/158, Loss: 0.028136011213064194\n",
      "Batch: 8/158, Loss: 0.024997945874929428\n",
      "Batch: 9/158, Loss: 0.023505443707108498\n",
      "Batch: 10/158, Loss: 0.03584601730108261\n",
      "Batch: 11/158, Loss: 0.0178530216217041\n",
      "Batch: 12/158, Loss: 0.017853155732154846\n",
      "Batch: 13/158, Loss: 0.028424017131328583\n",
      "Batch: 14/158, Loss: 0.023973166942596436\n",
      "Batch: 15/158, Loss: 0.025891728699207306\n",
      "Batch: 16/158, Loss: 0.017798230051994324\n",
      "Batch: 17/158, Loss: 0.028525931760668755\n",
      "Batch: 18/158, Loss: 0.023255838081240654\n",
      "Batch: 19/158, Loss: 0.02077583782374859\n",
      "Batch: 20/158, Loss: 0.032513149082660675\n",
      "Batch: 21/158, Loss: 0.019639473408460617\n",
      "Batch: 22/158, Loss: 0.03437904268503189\n",
      "Batch: 23/158, Loss: 0.028829872608184814\n",
      "Batch: 24/158, Loss: 0.028743639588356018\n",
      "Batch: 25/158, Loss: 0.017651647329330444\n",
      "Batch: 26/158, Loss: 0.018754027783870697\n",
      "Batch: 27/158, Loss: 0.023979857563972473\n",
      "Batch: 28/158, Loss: 0.029925914481282234\n",
      "Batch: 29/158, Loss: 0.0354171022772789\n",
      "Batch: 30/158, Loss: 0.027608221396803856\n",
      "Batch: 31/158, Loss: 0.03455744683742523\n",
      "Batch: 32/158, Loss: 0.02725507691502571\n",
      "Batch: 33/158, Loss: 0.02843577414751053\n",
      "Batch: 34/158, Loss: 0.023698830977082253\n",
      "Batch: 35/158, Loss: 0.011517656035721302\n",
      "Batch: 36/158, Loss: 0.024202551692724228\n",
      "Batch: 37/158, Loss: 0.012661885470151901\n",
      "Batch: 38/158, Loss: 0.027016611769795418\n",
      "Batch: 39/158, Loss: 0.021067393943667412\n",
      "Batch: 40/158, Loss: 0.022797761484980583\n",
      "Batch: 41/158, Loss: 0.027979519218206406\n",
      "Batch: 42/158, Loss: 0.01960195042192936\n",
      "Batch: 43/158, Loss: 0.019170919433236122\n",
      "Batch: 44/158, Loss: 0.024441326037049294\n",
      "Batch: 45/158, Loss: 0.021978415548801422\n",
      "Batch: 46/158, Loss: 0.02355348691344261\n",
      "Batch: 47/158, Loss: 0.023367196321487427\n",
      "Batch: 48/158, Loss: 0.025452880188822746\n",
      "Batch: 49/158, Loss: 0.017971640452742577\n",
      "Batch: 50/158, Loss: 0.017466921359300613\n",
      "Batch: 51/158, Loss: 0.018323050811886787\n",
      "Batch: 52/158, Loss: 0.0211038738489151\n",
      "Batch: 53/158, Loss: 0.02358849346637726\n",
      "Batch: 54/158, Loss: 0.020777644589543343\n",
      "Batch: 55/158, Loss: 0.017258290201425552\n",
      "Batch: 56/158, Loss: 0.012365935370326042\n",
      "Batch: 57/158, Loss: 0.015004520304501057\n",
      "Batch: 58/158, Loss: 0.015818199142813683\n",
      "Batch: 59/158, Loss: 0.013507219962775707\n",
      "Batch: 60/158, Loss: 0.0274430550634861\n",
      "Batch: 61/158, Loss: 0.018464989960193634\n",
      "Batch: 62/158, Loss: 0.021115880459547043\n",
      "Batch: 63/158, Loss: 0.026516014710068703\n",
      "Batch: 64/158, Loss: 0.020424749702215195\n",
      "Batch: 65/158, Loss: 0.01645285077393055\n",
      "Batch: 66/158, Loss: 0.012975146062672138\n",
      "Batch: 67/158, Loss: 0.014536920934915543\n",
      "Batch: 68/158, Loss: 0.01996009051799774\n",
      "Batch: 69/158, Loss: 0.01229490339756012\n",
      "Batch: 70/158, Loss: 0.013609418645501137\n",
      "Batch: 71/158, Loss: 0.01587490551173687\n",
      "Batch: 72/158, Loss: 0.014578648842871189\n",
      "Batch: 73/158, Loss: 0.037670064717531204\n",
      "Batch: 74/158, Loss: 0.012526809237897396\n",
      "Batch: 75/158, Loss: 0.01483632531017065\n",
      "Batch: 76/158, Loss: 0.0202387273311615\n",
      "Batch: 77/158, Loss: 0.01915164105594158\n",
      "Batch: 78/158, Loss: 0.025413833558559418\n",
      "Batch: 79/158, Loss: 0.01787506230175495\n",
      "Batch: 80/158, Loss: 0.019710058346390724\n",
      "Batch: 81/158, Loss: 0.01705280691385269\n",
      "Batch: 82/158, Loss: 0.030270732939243317\n",
      "Batch: 83/158, Loss: 0.010971627198159695\n",
      "Batch: 84/158, Loss: 0.024683181196451187\n",
      "Batch: 85/158, Loss: 0.014929242432117462\n",
      "Batch: 86/158, Loss: 0.01727711223065853\n",
      "Batch: 87/158, Loss: 0.016346193850040436\n",
      "Batch: 88/158, Loss: 0.01604629121720791\n",
      "Batch: 89/158, Loss: 0.012426682747900486\n",
      "Batch: 90/158, Loss: 0.019169017672538757\n",
      "Batch: 91/158, Loss: 0.016394836828112602\n",
      "Batch: 92/158, Loss: 0.018301675096154213\n",
      "Batch: 93/158, Loss: 0.015235909260809422\n",
      "Batch: 94/158, Loss: 0.019099662080407143\n",
      "Batch: 95/158, Loss: 0.026151923462748528\n",
      "Batch: 96/158, Loss: 0.013556607067584991\n",
      "Batch: 97/158, Loss: 0.013654205948114395\n",
      "Batch: 98/158, Loss: 0.008678045123815536\n",
      "Batch: 99/158, Loss: 0.01491671521216631\n",
      "Batch: 100/158, Loss: 0.00720662297680974\n",
      "Batch: 101/158, Loss: 0.008868283592164516\n",
      "Batch: 102/158, Loss: 0.027286149561405182\n",
      "Batch: 103/158, Loss: 0.012437279336154461\n",
      "Batch: 104/158, Loss: 0.013957275077700615\n",
      "Batch: 105/158, Loss: 0.01789410598576069\n",
      "Batch: 106/158, Loss: 0.01181604340672493\n",
      "Batch: 107/158, Loss: 0.020351102575659752\n",
      "Batch: 108/158, Loss: 0.009440484456717968\n",
      "Batch: 109/158, Loss: 0.01660618558526039\n",
      "Batch: 110/158, Loss: 0.015466862358152866\n",
      "Batch: 111/158, Loss: 0.009344086050987244\n",
      "Batch: 112/158, Loss: 0.011128236539661884\n",
      "Batch: 113/158, Loss: 0.016421066597104073\n",
      "Batch: 114/158, Loss: 0.013864581473171711\n",
      "Batch: 115/158, Loss: 0.015146611258387566\n",
      "Batch: 116/158, Loss: 0.01761901006102562\n",
      "Batch: 117/158, Loss: 0.011348271742463112\n",
      "Batch: 118/158, Loss: 0.012203353457152843\n",
      "Batch: 119/158, Loss: 0.02032526023685932\n",
      "Batch: 120/158, Loss: 0.02522023767232895\n",
      "Batch: 121/158, Loss: 0.016475265845656395\n",
      "Batch: 122/158, Loss: 0.015237611718475819\n",
      "Batch: 123/158, Loss: 0.014124729670584202\n",
      "Batch: 124/158, Loss: 0.016476690769195557\n",
      "Batch: 125/158, Loss: 0.00846956018358469\n",
      "Batch: 126/158, Loss: 0.016017084941267967\n",
      "Batch: 127/158, Loss: 0.012186398729681969\n",
      "Batch: 128/158, Loss: 0.012344712391495705\n",
      "Batch: 129/158, Loss: 0.011753607541322708\n",
      "Batch: 130/158, Loss: 0.00993907917290926\n",
      "Batch: 131/158, Loss: 0.011775542981922626\n",
      "Batch: 132/158, Loss: 0.011845475062727928\n",
      "Batch: 133/158, Loss: 0.010197602212429047\n",
      "Batch: 134/158, Loss: 0.008209099061787128\n",
      "Batch: 135/158, Loss: 0.011492649093270302\n",
      "Batch: 136/158, Loss: 0.012494885362684727\n",
      "Batch: 137/158, Loss: 0.01736014522612095\n",
      "Batch: 138/158, Loss: 0.00789120513945818\n",
      "Batch: 139/158, Loss: 0.00899413414299488\n",
      "Batch: 140/158, Loss: 0.022816840559244156\n",
      "Batch: 141/158, Loss: 0.008445294573903084\n",
      "Batch: 142/158, Loss: 0.012558918446302414\n",
      "Batch: 143/158, Loss: 0.008785136975347996\n",
      "Batch: 144/158, Loss: 0.011249015107750893\n",
      "Batch: 145/158, Loss: 0.00790468044579029\n",
      "Batch: 146/158, Loss: 0.009940103627741337\n",
      "Batch: 147/158, Loss: 0.011619169265031815\n",
      "Batch: 148/158, Loss: 0.010016348212957382\n",
      "Batch: 149/158, Loss: 0.009063057601451874\n",
      "Batch: 150/158, Loss: 0.006325048860162497\n",
      "Batch: 151/158, Loss: 0.010511599481105804\n",
      "Batch: 152/158, Loss: 0.007367544341832399\n",
      "Batch: 153/158, Loss: 0.011076360009610653\n",
      "Batch: 154/158, Loss: 0.010089724324643612\n",
      "Batch: 155/158, Loss: 0.010142912156879902\n",
      "Batch: 156/158, Loss: 0.011674834415316582\n",
      "Batch: 157/158, Loss: 0.008006229996681213\n",
      "Batch: 158/158, Loss: 0.013929187320172787\n",
      "Epoch: 3/10, Loss: 2.8738954630680382, Time: 75.89636468887329\n",
      "Batch: 1/158, Loss: 0.006094254087656736\n",
      "Batch: 2/158, Loss: 0.006383570376783609\n",
      "Batch: 3/158, Loss: 0.01037301030009985\n",
      "Batch: 4/158, Loss: 0.007777163293212652\n",
      "Batch: 5/158, Loss: 0.010376638732850552\n",
      "Batch: 6/158, Loss: 0.00784669816493988\n",
      "Batch: 7/158, Loss: 0.006503232289105654\n",
      "Batch: 8/158, Loss: 0.005947308614850044\n",
      "Batch: 9/158, Loss: 0.008019687607884407\n",
      "Batch: 10/158, Loss: 0.008810094557702541\n",
      "Batch: 11/158, Loss: 0.007155819330364466\n",
      "Batch: 12/158, Loss: 0.01229359582066536\n",
      "Batch: 13/158, Loss: 0.013414214365184307\n",
      "Batch: 14/158, Loss: 0.005522033665329218\n",
      "Batch: 15/158, Loss: 0.008849254809319973\n",
      "Batch: 16/158, Loss: 0.008562027476727962\n",
      "Batch: 17/158, Loss: 0.010896814987063408\n",
      "Batch: 18/158, Loss: 0.006938946433365345\n",
      "Batch: 19/158, Loss: 0.006961015053093433\n",
      "Batch: 20/158, Loss: 0.008927950635552406\n",
      "Batch: 21/158, Loss: 0.006209930405020714\n",
      "Batch: 22/158, Loss: 0.010456398129463196\n",
      "Batch: 23/158, Loss: 0.007287241518497467\n",
      "Batch: 24/158, Loss: 0.0076258378103375435\n",
      "Batch: 25/158, Loss: 0.007135600317269564\n",
      "Batch: 26/158, Loss: 0.004625051282346249\n",
      "Batch: 27/158, Loss: 0.006507495418190956\n",
      "Batch: 28/158, Loss: 0.011228613555431366\n",
      "Batch: 29/158, Loss: 0.013008191250264645\n",
      "Batch: 30/158, Loss: 0.004287820775061846\n",
      "Batch: 31/158, Loss: 0.006351935677230358\n",
      "Batch: 32/158, Loss: 0.006788633763790131\n",
      "Batch: 33/158, Loss: 0.007287579122930765\n",
      "Batch: 34/158, Loss: 0.00667579798027873\n",
      "Batch: 35/158, Loss: 0.00677216611802578\n",
      "Batch: 36/158, Loss: 0.012925163842737675\n",
      "Batch: 37/158, Loss: 0.006766409147530794\n",
      "Batch: 38/158, Loss: 0.00921366922557354\n",
      "Batch: 39/158, Loss: 0.005501036532223225\n",
      "Batch: 40/158, Loss: 0.008626415394246578\n",
      "Batch: 41/158, Loss: 0.010958741419017315\n",
      "Batch: 42/158, Loss: 0.0054710377007722855\n",
      "Batch: 43/158, Loss: 0.010860714130103588\n",
      "Batch: 44/158, Loss: 0.0060834805481135845\n",
      "Batch: 45/158, Loss: 0.008465204387903214\n",
      "Batch: 46/158, Loss: 0.005227587651461363\n",
      "Batch: 47/158, Loss: 0.00700531667098403\n",
      "Batch: 48/158, Loss: 0.004994275514036417\n",
      "Batch: 49/158, Loss: 0.006809580139815807\n",
      "Batch: 50/158, Loss: 0.007182869594544172\n",
      "Batch: 51/158, Loss: 0.005582190118730068\n",
      "Batch: 52/158, Loss: 0.004458914510905743\n",
      "Batch: 53/158, Loss: 0.006162744015455246\n",
      "Batch: 54/158, Loss: 0.008062531240284443\n",
      "Batch: 55/158, Loss: 0.005150152835994959\n",
      "Batch: 56/158, Loss: 0.007189057767391205\n",
      "Batch: 57/158, Loss: 0.006448902655392885\n",
      "Batch: 58/158, Loss: 0.0072196125984191895\n",
      "Batch: 59/158, Loss: 0.006538330111652613\n",
      "Batch: 60/158, Loss: 0.007384047377854586\n",
      "Batch: 61/158, Loss: 0.006634769029915333\n",
      "Batch: 62/158, Loss: 0.009273023344576359\n",
      "Batch: 63/158, Loss: 0.00670901732519269\n",
      "Batch: 64/158, Loss: 0.0065006595104932785\n",
      "Batch: 65/158, Loss: 0.011465686373412609\n",
      "Batch: 66/158, Loss: 0.006187949795275927\n",
      "Batch: 67/158, Loss: 0.0073638297617435455\n",
      "Batch: 68/158, Loss: 0.004982095677405596\n",
      "Batch: 69/158, Loss: 0.005852650851011276\n",
      "Batch: 70/158, Loss: 0.008073755539953709\n",
      "Batch: 71/158, Loss: 0.005677490029484034\n",
      "Batch: 72/158, Loss: 0.005889907479286194\n",
      "Batch: 73/158, Loss: 0.008639993146061897\n",
      "Batch: 74/158, Loss: 0.0036591030657291412\n",
      "Batch: 75/158, Loss: 0.007044100668281317\n",
      "Batch: 76/158, Loss: 0.010596213862299919\n",
      "Batch: 77/158, Loss: 0.006368613801896572\n",
      "Batch: 78/158, Loss: 0.005524300504475832\n",
      "Batch: 79/158, Loss: 0.006948629394173622\n",
      "Batch: 80/158, Loss: 0.00918008666485548\n",
      "Batch: 81/158, Loss: 0.007818952202796936\n",
      "Batch: 82/158, Loss: 0.007155151572078466\n",
      "Batch: 83/158, Loss: 0.004272862337529659\n",
      "Batch: 84/158, Loss: 0.005495857447385788\n",
      "Batch: 85/158, Loss: 0.005272102542221546\n",
      "Batch: 86/158, Loss: 0.00771348737180233\n",
      "Batch: 87/158, Loss: 0.007704632356762886\n",
      "Batch: 88/158, Loss: 0.008155177347362041\n",
      "Batch: 89/158, Loss: 0.0059101092629134655\n",
      "Batch: 90/158, Loss: 0.009169829078018665\n",
      "Batch: 91/158, Loss: 0.0037606926634907722\n",
      "Batch: 92/158, Loss: 0.00487982714548707\n",
      "Batch: 93/158, Loss: 0.007521882653236389\n",
      "Batch: 94/158, Loss: 0.004630709998309612\n",
      "Batch: 95/158, Loss: 0.011216381564736366\n",
      "Batch: 96/158, Loss: 0.005344634875655174\n",
      "Batch: 97/158, Loss: 0.004745678976178169\n",
      "Batch: 98/158, Loss: 0.005192800424993038\n",
      "Batch: 99/158, Loss: 0.005122565198689699\n",
      "Batch: 100/158, Loss: 0.005167732015252113\n",
      "Batch: 101/158, Loss: 0.01280359085649252\n",
      "Batch: 102/158, Loss: 0.004356348421424627\n",
      "Batch: 103/158, Loss: 0.009749344550073147\n",
      "Batch: 104/158, Loss: 0.004452082794159651\n",
      "Batch: 105/158, Loss: 0.010848830454051495\n",
      "Batch: 106/158, Loss: 0.008238548412919044\n",
      "Batch: 107/158, Loss: 0.004237977787852287\n",
      "Batch: 108/158, Loss: 0.004929776303470135\n",
      "Batch: 109/158, Loss: 0.006810891442000866\n",
      "Batch: 110/158, Loss: 0.003922486212104559\n",
      "Batch: 111/158, Loss: 0.005529699381440878\n",
      "Batch: 112/158, Loss: 0.003910253290086985\n",
      "Batch: 113/158, Loss: 0.003978372551500797\n",
      "Batch: 114/158, Loss: 0.005718272645026445\n",
      "Batch: 115/158, Loss: 0.006429168861359358\n",
      "Batch: 116/158, Loss: 0.00399041036143899\n",
      "Batch: 117/158, Loss: 0.006815461907535791\n",
      "Batch: 118/158, Loss: 0.004657014273107052\n",
      "Batch: 119/158, Loss: 0.006880599074065685\n",
      "Batch: 120/158, Loss: 0.005692577455192804\n",
      "Batch: 121/158, Loss: 0.014962460845708847\n",
      "Batch: 122/158, Loss: 0.00470293965190649\n",
      "Batch: 123/158, Loss: 0.007672863081097603\n",
      "Batch: 124/158, Loss: 0.00750734144821763\n",
      "Batch: 125/158, Loss: 0.00529858423396945\n",
      "Batch: 126/158, Loss: 0.005423540249466896\n",
      "Batch: 127/158, Loss: 0.005225071217864752\n",
      "Batch: 128/158, Loss: 0.006041104439646006\n",
      "Batch: 129/158, Loss: 0.005831646267324686\n",
      "Batch: 130/158, Loss: 0.004167477134615183\n",
      "Batch: 131/158, Loss: 0.005802359897643328\n",
      "Batch: 132/158, Loss: 0.003814299590885639\n",
      "Batch: 133/158, Loss: 0.010770854540169239\n",
      "Batch: 134/158, Loss: 0.0090980539098382\n",
      "Batch: 135/158, Loss: 0.003704050090163946\n",
      "Batch: 136/158, Loss: 0.0043926234357059\n",
      "Batch: 137/158, Loss: 0.004794656299054623\n",
      "Batch: 138/158, Loss: 0.0047546131536364555\n",
      "Batch: 139/158, Loss: 0.005194659344851971\n",
      "Batch: 140/158, Loss: 0.004759961739182472\n",
      "Batch: 141/158, Loss: 0.007310345768928528\n",
      "Batch: 142/158, Loss: 0.006616802420467138\n",
      "Batch: 143/158, Loss: 0.007392051629722118\n",
      "Batch: 144/158, Loss: 0.0032341608311980963\n",
      "Batch: 145/158, Loss: 0.006266695447266102\n",
      "Batch: 146/158, Loss: 0.0037015716079622507\n",
      "Batch: 147/158, Loss: 0.004638671409338713\n",
      "Batch: 148/158, Loss: 0.005587375722825527\n",
      "Batch: 149/158, Loss: 0.004362018313258886\n",
      "Batch: 150/158, Loss: 0.003983056638389826\n",
      "Batch: 151/158, Loss: 0.011585894040763378\n",
      "Batch: 152/158, Loss: 0.003676555585116148\n",
      "Batch: 153/158, Loss: 0.006080451421439648\n",
      "Batch: 154/158, Loss: 0.00478077819570899\n",
      "Batch: 155/158, Loss: 0.0034749351907521486\n",
      "Batch: 156/158, Loss: 0.003568887710571289\n",
      "Batch: 157/158, Loss: 0.0056493584997951984\n",
      "Batch: 158/158, Loss: 0.004874318838119507\n",
      "Epoch: 4/10, Loss: 1.073667348595336, Time: 70.63374495506287\n",
      "Batch: 1/158, Loss: 0.00378879951313138\n",
      "Batch: 2/158, Loss: 0.004546539392322302\n",
      "Batch: 3/158, Loss: 0.004032940603792667\n",
      "Batch: 4/158, Loss: 0.009076557122170925\n",
      "Batch: 5/158, Loss: 0.004154179710894823\n",
      "Batch: 6/158, Loss: 0.0033295887988060713\n",
      "Batch: 7/158, Loss: 0.004403341095894575\n",
      "Batch: 8/158, Loss: 0.0033341117668896914\n",
      "Batch: 9/158, Loss: 0.0031671584583818913\n",
      "Batch: 10/158, Loss: 0.0036735821049660444\n",
      "Batch: 11/158, Loss: 0.003480183891952038\n",
      "Batch: 12/158, Loss: 0.0035741333849728107\n",
      "Batch: 13/158, Loss: 0.003925778903067112\n",
      "Batch: 14/158, Loss: 0.0038465899415314198\n",
      "Batch: 15/158, Loss: 0.0028458356391638517\n",
      "Batch: 16/158, Loss: 0.003954638727009296\n",
      "Batch: 17/158, Loss: 0.004573469515889883\n",
      "Batch: 18/158, Loss: 0.0030141768511384726\n",
      "Batch: 19/158, Loss: 0.0033197139855474234\n",
      "Batch: 20/158, Loss: 0.004167979117482901\n",
      "Batch: 21/158, Loss: 0.002628199988976121\n",
      "Batch: 22/158, Loss: 0.003270466811954975\n",
      "Batch: 23/158, Loss: 0.003289325162768364\n",
      "Batch: 24/158, Loss: 0.0035175145603716373\n",
      "Batch: 25/158, Loss: 0.003120382549241185\n",
      "Batch: 26/158, Loss: 0.003592759370803833\n",
      "Batch: 27/158, Loss: 0.004387155640870333\n",
      "Batch: 28/158, Loss: 0.0029081767424941063\n",
      "Batch: 29/158, Loss: 0.004146276507526636\n",
      "Batch: 30/158, Loss: 0.002817683620378375\n",
      "Batch: 31/158, Loss: 0.005098148714751005\n",
      "Batch: 32/158, Loss: 0.004659190773963928\n",
      "Batch: 33/158, Loss: 0.003773751202970743\n",
      "Batch: 34/158, Loss: 0.003370780497789383\n",
      "Batch: 35/158, Loss: 0.004563753493130207\n",
      "Batch: 36/158, Loss: 0.0033414498902857304\n",
      "Batch: 37/158, Loss: 0.003935862332582474\n",
      "Batch: 38/158, Loss: 0.004515090025961399\n",
      "Batch: 39/158, Loss: 0.0037340279668569565\n",
      "Batch: 40/158, Loss: 0.003485123161226511\n",
      "Batch: 41/158, Loss: 0.003289391752332449\n",
      "Batch: 42/158, Loss: 0.00315892999060452\n",
      "Batch: 43/158, Loss: 0.0027237976901233196\n",
      "Batch: 44/158, Loss: 0.0033067241311073303\n",
      "Batch: 45/158, Loss: 0.003811294911429286\n",
      "Batch: 46/158, Loss: 0.0056219580583274364\n",
      "Batch: 47/158, Loss: 0.003624844131991267\n",
      "Batch: 48/158, Loss: 0.0031297754030674696\n",
      "Batch: 49/158, Loss: 0.003915717359632254\n",
      "Batch: 50/158, Loss: 0.004115065094083548\n",
      "Batch: 51/158, Loss: 0.0029888481367379427\n",
      "Batch: 52/158, Loss: 0.0035212961956858635\n",
      "Batch: 53/158, Loss: 0.002940861042588949\n",
      "Batch: 54/158, Loss: 0.0031606501433998346\n",
      "Batch: 55/158, Loss: 0.005627835169434547\n",
      "Batch: 56/158, Loss: 0.0031928985845297575\n",
      "Batch: 57/158, Loss: 0.003444094443693757\n",
      "Batch: 58/158, Loss: 0.003067499026656151\n",
      "Batch: 59/158, Loss: 0.002429001033306122\n",
      "Batch: 60/158, Loss: 0.0031317316461354494\n",
      "Batch: 61/158, Loss: 0.002512172097340226\n",
      "Batch: 62/158, Loss: 0.003286163555458188\n",
      "Batch: 63/158, Loss: 0.0024706879630684853\n",
      "Batch: 64/158, Loss: 0.002756673377007246\n",
      "Batch: 65/158, Loss: 0.0034648478031158447\n",
      "Batch: 66/158, Loss: 0.0034397109411656857\n",
      "Batch: 67/158, Loss: 0.004093443509191275\n",
      "Batch: 68/158, Loss: 0.0028349768836051226\n",
      "Batch: 69/158, Loss: 0.003485406981781125\n",
      "Batch: 70/158, Loss: 0.004689139313995838\n",
      "Batch: 71/158, Loss: 0.0037904013879597187\n",
      "Batch: 72/158, Loss: 0.004848914686590433\n",
      "Batch: 73/158, Loss: 0.0025529961567372084\n",
      "Batch: 74/158, Loss: 0.0031148542184382677\n",
      "Batch: 75/158, Loss: 0.0034034859854727983\n",
      "Batch: 76/158, Loss: 0.004470603074878454\n",
      "Batch: 77/158, Loss: 0.002628639340400696\n",
      "Batch: 78/158, Loss: 0.002808526623994112\n",
      "Batch: 79/158, Loss: 0.0030680459458380938\n",
      "Batch: 80/158, Loss: 0.004535182379186153\n",
      "Batch: 81/158, Loss: 0.0027179140597581863\n",
      "Batch: 82/158, Loss: 0.002650367096066475\n",
      "Batch: 83/158, Loss: 0.0025401778984814882\n",
      "Batch: 84/158, Loss: 0.0027269620914012194\n",
      "Batch: 85/158, Loss: 0.002841186709702015\n",
      "Batch: 86/158, Loss: 0.0035469746217131615\n",
      "Batch: 87/158, Loss: 0.003276099916547537\n",
      "Batch: 88/158, Loss: 0.004721904639154673\n",
      "Batch: 89/158, Loss: 0.004969601985067129\n",
      "Batch: 90/158, Loss: 0.0043114121071994305\n",
      "Batch: 91/158, Loss: 0.002696278737857938\n",
      "Batch: 92/158, Loss: 0.0028225521091371775\n",
      "Batch: 93/158, Loss: 0.0026950573083013296\n",
      "Batch: 94/158, Loss: 0.0037108655087649822\n",
      "Batch: 95/158, Loss: 0.003519637743011117\n",
      "Batch: 96/158, Loss: 0.0024706709664314985\n",
      "Batch: 97/158, Loss: 0.002446776255965233\n",
      "Batch: 98/158, Loss: 0.004969879984855652\n",
      "Batch: 99/158, Loss: 0.0023454453330487013\n",
      "Batch: 100/158, Loss: 0.002625785069540143\n",
      "Batch: 101/158, Loss: 0.002883604494854808\n",
      "Batch: 102/158, Loss: 0.003600670490413904\n",
      "Batch: 103/158, Loss: 0.0029030186124145985\n",
      "Batch: 104/158, Loss: 0.0023515308275818825\n",
      "Batch: 105/158, Loss: 0.00262074894271791\n",
      "Batch: 106/158, Loss: 0.0030281248036772013\n",
      "Batch: 107/158, Loss: 0.002575642429292202\n",
      "Batch: 108/158, Loss: 0.0024055619724094868\n",
      "Batch: 109/158, Loss: 0.002737855538725853\n",
      "Batch: 110/158, Loss: 0.002549470169469714\n",
      "Batch: 111/158, Loss: 0.004021035972982645\n",
      "Batch: 112/158, Loss: 0.004882774315774441\n",
      "Batch: 113/158, Loss: 0.0026237214915454388\n",
      "Batch: 114/158, Loss: 0.003971248399466276\n",
      "Batch: 115/158, Loss: 0.002720128046348691\n",
      "Batch: 116/158, Loss: 0.0034888191148638725\n",
      "Batch: 117/158, Loss: 0.003445772221311927\n",
      "Batch: 118/158, Loss: 0.0028526505921036005\n",
      "Batch: 119/158, Loss: 0.0024068187922239304\n",
      "Batch: 120/158, Loss: 0.0023235681001096964\n",
      "Batch: 121/158, Loss: 0.002998648677021265\n",
      "Batch: 122/158, Loss: 0.0031688851304352283\n",
      "Batch: 123/158, Loss: 0.0025793761014938354\n",
      "Batch: 124/158, Loss: 0.004273229744285345\n",
      "Batch: 125/158, Loss: 0.002661221893504262\n",
      "Batch: 126/158, Loss: 0.0026716110296547413\n",
      "Batch: 127/158, Loss: 0.002501368522644043\n",
      "Batch: 128/158, Loss: 0.0023910589516162872\n",
      "Batch: 129/158, Loss: 0.002246624557301402\n",
      "Batch: 130/158, Loss: 0.003887817030772567\n",
      "Batch: 131/158, Loss: 0.0021984241902828217\n",
      "Batch: 132/158, Loss: 0.0035141543485224247\n",
      "Batch: 133/158, Loss: 0.00237847282551229\n",
      "Batch: 134/158, Loss: 0.003126357449218631\n",
      "Batch: 135/158, Loss: 0.003061457769945264\n",
      "Batch: 136/158, Loss: 0.002334265736863017\n",
      "Batch: 137/158, Loss: 0.003230549395084381\n",
      "Batch: 138/158, Loss: 0.0027057258412241936\n",
      "Batch: 139/158, Loss: 0.0026902644895017147\n",
      "Batch: 140/158, Loss: 0.0050008115358650684\n",
      "Batch: 141/158, Loss: 0.002718328731134534\n",
      "Batch: 142/158, Loss: 0.004593484569340944\n",
      "Batch: 143/158, Loss: 0.0024877486284822226\n",
      "Batch: 144/158, Loss: 0.0027157755102962255\n",
      "Batch: 145/158, Loss: 0.002015109406784177\n",
      "Batch: 146/158, Loss: 0.002749516163021326\n",
      "Batch: 147/158, Loss: 0.002758219139650464\n",
      "Batch: 148/158, Loss: 0.0027169666718691587\n",
      "Batch: 149/158, Loss: 0.00295940600335598\n",
      "Batch: 150/158, Loss: 0.0029109155293554068\n",
      "Batch: 151/158, Loss: 0.0025572532322257757\n",
      "Batch: 152/158, Loss: 0.002530844183638692\n",
      "Batch: 153/158, Loss: 0.002672214526683092\n",
      "Batch: 154/158, Loss: 0.0028166703414171934\n",
      "Batch: 155/158, Loss: 0.0029696705751121044\n",
      "Batch: 156/158, Loss: 0.0022327504120767117\n",
      "Batch: 157/158, Loss: 0.002408441621810198\n",
      "Batch: 158/158, Loss: 0.002878328785300255\n",
      "Epoch: 5/10, Loss: 0.5271317884325981, Time: 71.28430008888245\n",
      "Batch: 1/158, Loss: 0.002623111940920353\n",
      "Batch: 2/158, Loss: 0.0023585143499076366\n",
      "Batch: 3/158, Loss: 0.002114291535690427\n",
      "Batch: 4/158, Loss: 0.0021493700332939625\n",
      "Batch: 5/158, Loss: 0.0018365818541496992\n",
      "Batch: 6/158, Loss: 0.0018914040410891175\n",
      "Batch: 7/158, Loss: 0.0022826732601970434\n",
      "Batch: 8/158, Loss: 0.0019867781084030867\n",
      "Batch: 9/158, Loss: 0.0025235083885490894\n",
      "Batch: 10/158, Loss: 0.00245583220385015\n",
      "Batch: 11/158, Loss: 0.0020946196746081114\n",
      "Batch: 12/158, Loss: 0.0022691816557198763\n",
      "Batch: 13/158, Loss: 0.002700662473216653\n",
      "Batch: 14/158, Loss: 0.0018942402675747871\n",
      "Batch: 15/158, Loss: 0.0016969788121059537\n",
      "Batch: 16/158, Loss: 0.0022645883727818727\n",
      "Batch: 17/158, Loss: 0.00219919066876173\n",
      "Batch: 18/158, Loss: 0.002292365301400423\n",
      "Batch: 19/158, Loss: 0.002160366391763091\n",
      "Batch: 20/158, Loss: 0.0024612261913716793\n",
      "Batch: 21/158, Loss: 0.0020107808522880077\n",
      "Batch: 22/158, Loss: 0.002155795693397522\n",
      "Batch: 23/158, Loss: 0.0023732197005301714\n",
      "Batch: 24/158, Loss: 0.0022616437636315823\n",
      "Batch: 25/158, Loss: 0.00215362710878253\n",
      "Batch: 26/158, Loss: 0.0025770512875169516\n",
      "Batch: 27/158, Loss: 0.002783415839076042\n",
      "Batch: 28/158, Loss: 0.0021594080608338118\n",
      "Batch: 29/158, Loss: 0.0022265338338911533\n",
      "Batch: 30/158, Loss: 0.002463450189679861\n",
      "Batch: 31/158, Loss: 0.002412916859611869\n",
      "Batch: 32/158, Loss: 0.0026649979408830404\n",
      "Batch: 33/158, Loss: 0.0017851609736680984\n",
      "Batch: 34/158, Loss: 0.0021190422121435404\n",
      "Batch: 35/158, Loss: 0.002171256812289357\n",
      "Batch: 36/158, Loss: 0.0027146830689162016\n",
      "Batch: 37/158, Loss: 0.002193374326452613\n",
      "Batch: 38/158, Loss: 0.002095982898026705\n",
      "Batch: 39/158, Loss: 0.0023968254681676626\n",
      "Batch: 40/158, Loss: 0.0022856879513710737\n",
      "Batch: 41/158, Loss: 0.0022568542044609785\n",
      "Batch: 42/158, Loss: 0.002044389257207513\n",
      "Batch: 43/158, Loss: 0.0021362712141126394\n",
      "Batch: 44/158, Loss: 0.002273981925100088\n",
      "Batch: 45/158, Loss: 0.002041484462097287\n",
      "Batch: 46/158, Loss: 0.0020312792621552944\n",
      "Batch: 47/158, Loss: 0.0021626220550388098\n",
      "Batch: 48/158, Loss: 0.0029175900854170322\n",
      "Batch: 49/158, Loss: 0.002211540238931775\n",
      "Batch: 50/158, Loss: 0.001708212890662253\n",
      "Batch: 51/158, Loss: 0.002335852710530162\n",
      "Batch: 52/158, Loss: 0.0017991885542869568\n",
      "Batch: 53/158, Loss: 0.001955841202288866\n",
      "Batch: 54/158, Loss: 0.0021161064505577087\n",
      "Batch: 55/158, Loss: 0.0018441074062138796\n",
      "Batch: 56/158, Loss: 0.0017403106903657317\n",
      "Batch: 57/158, Loss: 0.0016277112299576402\n",
      "Batch: 58/158, Loss: 0.0027100113220512867\n",
      "Batch: 59/158, Loss: 0.002005739137530327\n",
      "Batch: 60/158, Loss: 0.002456239890307188\n",
      "Batch: 61/158, Loss: 0.002019672654569149\n",
      "Batch: 62/158, Loss: 0.0020482796244323254\n",
      "Batch: 63/158, Loss: 0.0017154371598735452\n",
      "Batch: 64/158, Loss: 0.0024160670582205057\n",
      "Batch: 65/158, Loss: 0.0020980846602469683\n",
      "Batch: 66/158, Loss: 0.0014959918335080147\n",
      "Batch: 67/158, Loss: 0.0017187792109325528\n",
      "Batch: 68/158, Loss: 0.0020433468744158745\n",
      "Batch: 69/158, Loss: 0.0021972721442580223\n",
      "Batch: 70/158, Loss: 0.00195412989705801\n",
      "Batch: 71/158, Loss: 0.002082447288557887\n",
      "Batch: 72/158, Loss: 0.002059503924101591\n",
      "Batch: 73/158, Loss: 0.0021612404379993677\n",
      "Batch: 74/158, Loss: 0.0030741128139197826\n",
      "Batch: 75/158, Loss: 0.002165350830182433\n",
      "Batch: 76/158, Loss: 0.0020713647827506065\n",
      "Batch: 77/158, Loss: 0.002153161447495222\n",
      "Batch: 78/158, Loss: 0.0020167995244264603\n",
      "Batch: 79/158, Loss: 0.0016986456466838717\n",
      "Batch: 80/158, Loss: 0.0020503553096204996\n",
      "Batch: 81/158, Loss: 0.002677739365026355\n",
      "Batch: 82/158, Loss: 0.002123728161677718\n",
      "Batch: 83/158, Loss: 0.0019374036928638816\n",
      "Batch: 84/158, Loss: 0.002005809685215354\n",
      "Batch: 85/158, Loss: 0.0019834903068840504\n",
      "Batch: 86/158, Loss: 0.0019234165083616972\n",
      "Batch: 87/158, Loss: 0.001790112815797329\n",
      "Batch: 88/158, Loss: 0.0025697608944028616\n",
      "Batch: 89/158, Loss: 0.0020261439494788647\n",
      "Batch: 90/158, Loss: 0.002136681694537401\n",
      "Batch: 91/158, Loss: 0.0018075788393616676\n",
      "Batch: 92/158, Loss: 0.0014566935133188963\n",
      "Batch: 93/158, Loss: 0.0020396332256495953\n",
      "Batch: 94/158, Loss: 0.0018082434544339776\n",
      "Batch: 95/158, Loss: 0.001757963327690959\n",
      "Batch: 96/158, Loss: 0.0014666158240288496\n",
      "Batch: 97/158, Loss: 0.0019128188723698258\n",
      "Batch: 98/158, Loss: 0.002416193252429366\n",
      "Batch: 99/158, Loss: 0.0020095703657716513\n",
      "Batch: 100/158, Loss: 0.0017000503139570355\n",
      "Batch: 101/158, Loss: 0.002047312678769231\n",
      "Batch: 102/158, Loss: 0.002188929356634617\n",
      "Batch: 103/158, Loss: 0.0020321174524724483\n",
      "Batch: 104/158, Loss: 0.0019747770857065916\n",
      "Batch: 105/158, Loss: 0.002370005240663886\n",
      "Batch: 106/158, Loss: 0.002111783716827631\n",
      "Batch: 107/158, Loss: 0.0016785149928182364\n",
      "Batch: 108/158, Loss: 0.0020460106898099184\n",
      "Batch: 109/158, Loss: 0.0020696455612778664\n",
      "Batch: 110/158, Loss: 0.0015279820654541254\n",
      "Batch: 111/158, Loss: 0.0017878038343042135\n",
      "Batch: 112/158, Loss: 0.002529907040297985\n",
      "Batch: 113/158, Loss: 0.001947822398506105\n",
      "Batch: 114/158, Loss: 0.0019063256913796067\n",
      "Batch: 115/158, Loss: 0.0015119535382837057\n",
      "Batch: 116/158, Loss: 0.0018192764837294817\n",
      "Batch: 117/158, Loss: 0.001820442033931613\n",
      "Batch: 118/158, Loss: 0.0015639772173017263\n",
      "Batch: 119/158, Loss: 0.0026131842751055956\n",
      "Batch: 120/158, Loss: 0.002055543940514326\n",
      "Batch: 121/158, Loss: 0.0017180354334414005\n",
      "Batch: 122/158, Loss: 0.0017868814757093787\n",
      "Batch: 123/158, Loss: 0.0014997330727055669\n",
      "Batch: 124/158, Loss: 0.002103593200445175\n",
      "Batch: 125/158, Loss: 0.0017081261612474918\n",
      "Batch: 126/158, Loss: 0.0018766082357615232\n",
      "Batch: 127/158, Loss: 0.0018134093843400478\n",
      "Batch: 128/158, Loss: 0.0016496233874931931\n",
      "Batch: 129/158, Loss: 0.0021545232739299536\n",
      "Batch: 130/158, Loss: 0.00192559405695647\n",
      "Batch: 131/158, Loss: 0.0021039133425801992\n",
      "Batch: 132/158, Loss: 0.002681581536307931\n",
      "Batch: 133/158, Loss: 0.0016766703920438886\n",
      "Batch: 134/158, Loss: 0.001986874733120203\n",
      "Batch: 135/158, Loss: 0.0017188354395329952\n",
      "Batch: 136/158, Loss: 0.00210165255703032\n",
      "Batch: 137/158, Loss: 0.0018652386497706175\n",
      "Batch: 138/158, Loss: 0.002158323535695672\n",
      "Batch: 139/158, Loss: 0.001771148992702365\n",
      "Batch: 140/158, Loss: 0.0023932678159326315\n",
      "Batch: 141/158, Loss: 0.0020379233174026012\n",
      "Batch: 142/158, Loss: 0.001759498380124569\n",
      "Batch: 143/158, Loss: 0.0016815254930406809\n",
      "Batch: 144/158, Loss: 0.0016038920730352402\n",
      "Batch: 145/158, Loss: 0.0021022201981395483\n",
      "Batch: 146/158, Loss: 0.0019777892157435417\n",
      "Batch: 147/158, Loss: 0.0016015181317925453\n",
      "Batch: 148/158, Loss: 0.0018436415120959282\n",
      "Batch: 149/158, Loss: 0.0017938008531928062\n",
      "Batch: 150/158, Loss: 0.0018434226512908936\n",
      "Batch: 151/158, Loss: 0.0018209898844361305\n",
      "Batch: 152/158, Loss: 0.0015663955127820373\n",
      "Batch: 153/158, Loss: 0.001414287369698286\n",
      "Batch: 154/158, Loss: 0.0020709880627691746\n",
      "Batch: 155/158, Loss: 0.001507795648649335\n",
      "Batch: 156/158, Loss: 0.002281568944454193\n",
      "Batch: 157/158, Loss: 0.001686088158749044\n",
      "Batch: 158/158, Loss: 0.0016426860820502043\n",
      "Epoch: 6/10, Loss: 0.3239583472022787, Time: 71.07019591331482\n",
      "Batch: 1/158, Loss: 0.0015566812362521887\n",
      "Batch: 2/158, Loss: 0.00142459396738559\n",
      "Batch: 3/158, Loss: 0.0013449861435219646\n",
      "Batch: 4/158, Loss: 0.001805403851903975\n",
      "Batch: 5/158, Loss: 0.0017577146645635366\n",
      "Batch: 6/158, Loss: 0.0019772490486502647\n",
      "Batch: 7/158, Loss: 0.0018104069167748094\n",
      "Batch: 8/158, Loss: 0.0014250718522816896\n",
      "Batch: 9/158, Loss: 0.0015036119148135185\n",
      "Batch: 10/158, Loss: 0.001889518927782774\n",
      "Batch: 11/158, Loss: 0.0017924811691045761\n",
      "Batch: 12/158, Loss: 0.0013988668797537684\n",
      "Batch: 13/158, Loss: 0.001554892398416996\n",
      "Batch: 14/158, Loss: 0.0016310419887304306\n",
      "Batch: 15/158, Loss: 0.0014956197701394558\n",
      "Batch: 16/158, Loss: 0.0014147864421829581\n",
      "Batch: 17/158, Loss: 0.0018874830566346645\n",
      "Batch: 18/158, Loss: 0.0015968726947903633\n",
      "Batch: 19/158, Loss: 0.0015088735381141305\n",
      "Batch: 20/158, Loss: 0.001472814125008881\n",
      "Batch: 21/158, Loss: 0.0016071256250143051\n",
      "Batch: 22/158, Loss: 0.0016895598964765668\n",
      "Batch: 23/158, Loss: 0.0013687697937712073\n",
      "Batch: 24/158, Loss: 0.0015261240769177675\n",
      "Batch: 25/158, Loss: 0.0014951527118682861\n",
      "Batch: 26/158, Loss: 0.0013935910537838936\n",
      "Batch: 27/158, Loss: 0.0014376980252563953\n",
      "Batch: 28/158, Loss: 0.0017235307022929192\n",
      "Batch: 29/158, Loss: 0.0018176044104620814\n",
      "Batch: 30/158, Loss: 0.0014605727046728134\n",
      "Batch: 31/158, Loss: 0.0016175125492736697\n",
      "Batch: 32/158, Loss: 0.0012118280865252018\n",
      "Batch: 33/158, Loss: 0.0016242427518591285\n",
      "Batch: 34/158, Loss: 0.0015811711782589555\n",
      "Batch: 35/158, Loss: 0.0013593562180176377\n",
      "Batch: 36/158, Loss: 0.001603477168828249\n",
      "Batch: 37/158, Loss: 0.0014258385635912418\n",
      "Batch: 38/158, Loss: 0.0016021325718611479\n",
      "Batch: 39/158, Loss: 0.001407324569299817\n",
      "Batch: 40/158, Loss: 0.0011079040123149753\n",
      "Batch: 41/158, Loss: 0.0018270297441631556\n",
      "Batch: 42/158, Loss: 0.0017431798623874784\n",
      "Batch: 43/158, Loss: 0.0016590289305895567\n",
      "Batch: 44/158, Loss: 0.0015633967705070972\n",
      "Batch: 45/158, Loss: 0.001504754414781928\n",
      "Batch: 46/158, Loss: 0.0016816072165966034\n",
      "Batch: 47/158, Loss: 0.0015823525609448552\n",
      "Batch: 48/158, Loss: 0.0012934309197589755\n",
      "Batch: 49/158, Loss: 0.0016342306043952703\n",
      "Batch: 50/158, Loss: 0.001399078406393528\n",
      "Batch: 51/158, Loss: 0.0011923681013286114\n",
      "Batch: 52/158, Loss: 0.001878354582004249\n",
      "Batch: 53/158, Loss: 0.0013850663090124726\n",
      "Batch: 54/158, Loss: 0.0016045625088736415\n",
      "Batch: 55/158, Loss: 0.0013719366397708654\n",
      "Batch: 56/158, Loss: 0.0014475748175755143\n",
      "Batch: 57/158, Loss: 0.0016503131482750177\n",
      "Batch: 58/158, Loss: 0.0016235706862062216\n",
      "Batch: 59/158, Loss: 0.0012437746627256274\n",
      "Batch: 60/158, Loss: 0.0020902459509670734\n",
      "Batch: 61/158, Loss: 0.0015409612096846104\n",
      "Batch: 62/158, Loss: 0.0013392324326559901\n",
      "Batch: 63/158, Loss: 0.0014360143104568124\n",
      "Batch: 64/158, Loss: 0.0014882012037560344\n",
      "Batch: 65/158, Loss: 0.0015579171013087034\n",
      "Batch: 66/158, Loss: 0.001315774628892541\n",
      "Batch: 67/158, Loss: 0.001341553288511932\n",
      "Batch: 68/158, Loss: 0.0013876777375116944\n",
      "Batch: 69/158, Loss: 0.0013049354311078787\n",
      "Batch: 70/158, Loss: 0.0014784032246097922\n",
      "Batch: 71/158, Loss: 0.0014498959062620997\n",
      "Batch: 72/158, Loss: 0.0012929791118949652\n",
      "Batch: 73/158, Loss: 0.0014972465578466654\n",
      "Batch: 74/158, Loss: 0.0014959498075768352\n",
      "Batch: 75/158, Loss: 0.0012138766469433904\n",
      "Batch: 76/158, Loss: 0.0016881346236914396\n",
      "Batch: 77/158, Loss: 0.0015201331116259098\n",
      "Batch: 78/158, Loss: 0.0014306969242170453\n",
      "Batch: 79/158, Loss: 0.0015592784620821476\n",
      "Batch: 80/158, Loss: 0.0014750543050467968\n",
      "Batch: 81/158, Loss: 0.0010908531257882714\n",
      "Batch: 82/158, Loss: 0.0013772069942206144\n",
      "Batch: 83/158, Loss: 0.0014641578309237957\n",
      "Batch: 84/158, Loss: 0.0011733288411051035\n",
      "Batch: 85/158, Loss: 0.0015003150328993797\n",
      "Batch: 86/158, Loss: 0.0013305750908330083\n",
      "Batch: 87/158, Loss: 0.0011496992083266377\n",
      "Batch: 88/158, Loss: 0.0012946106726303697\n",
      "Batch: 89/158, Loss: 0.0014195876428857446\n",
      "Batch: 90/158, Loss: 0.0015053305542096496\n",
      "Batch: 91/158, Loss: 0.0017193922540172935\n",
      "Batch: 92/158, Loss: 0.0016061972128227353\n",
      "Batch: 93/158, Loss: 0.0015145273646339774\n",
      "Batch: 94/158, Loss: 0.001205582870170474\n",
      "Batch: 95/158, Loss: 0.0014347315300256014\n",
      "Batch: 96/158, Loss: 0.0013036461314186454\n",
      "Batch: 97/158, Loss: 0.0014961444539949298\n",
      "Batch: 98/158, Loss: 0.0014708373928442597\n",
      "Batch: 99/158, Loss: 0.001285378122702241\n",
      "Batch: 100/158, Loss: 0.0013369355583563447\n",
      "Batch: 101/158, Loss: 0.0010624728165566921\n",
      "Batch: 102/158, Loss: 0.0014299767790362239\n",
      "Batch: 103/158, Loss: 0.0009496794082224369\n",
      "Batch: 104/158, Loss: 0.0013977076159790158\n",
      "Batch: 105/158, Loss: 0.0014327657409012318\n",
      "Batch: 106/158, Loss: 0.0015532287070527673\n",
      "Batch: 107/158, Loss: 0.0012720344820991158\n",
      "Batch: 108/158, Loss: 0.0010573130566626787\n",
      "Batch: 109/158, Loss: 0.0013326294720172882\n",
      "Batch: 110/158, Loss: 0.0013635766226798296\n",
      "Batch: 111/158, Loss: 0.0014348756521940231\n",
      "Batch: 112/158, Loss: 0.0016885107615962625\n",
      "Batch: 113/158, Loss: 0.0011427360586822033\n",
      "Batch: 114/158, Loss: 0.0012860409915447235\n",
      "Batch: 115/158, Loss: 0.001586488331668079\n",
      "Batch: 116/158, Loss: 0.0015021285507827997\n",
      "Batch: 117/158, Loss: 0.0016390690580010414\n",
      "Batch: 118/158, Loss: 0.0016052679857239127\n",
      "Batch: 119/158, Loss: 0.0014481215039268136\n",
      "Batch: 120/158, Loss: 0.0014447270659729838\n",
      "Batch: 121/158, Loss: 0.0015135976718738675\n",
      "Batch: 122/158, Loss: 0.0012149661779403687\n",
      "Batch: 123/158, Loss: 0.0013441082555800676\n",
      "Batch: 124/158, Loss: 0.0012636827304959297\n",
      "Batch: 125/158, Loss: 0.001581240096129477\n",
      "Batch: 126/158, Loss: 0.001501693157479167\n",
      "Batch: 127/158, Loss: 0.0013931678840890527\n",
      "Batch: 128/158, Loss: 0.0014038445660844445\n",
      "Batch: 129/158, Loss: 0.0015216037863865495\n",
      "Batch: 130/158, Loss: 0.0012212800793349743\n",
      "Batch: 131/158, Loss: 0.001301381504163146\n",
      "Batch: 132/158, Loss: 0.0013962555676698685\n",
      "Batch: 133/158, Loss: 0.001390791847370565\n",
      "Batch: 134/158, Loss: 0.0013312287628650665\n",
      "Batch: 135/158, Loss: 0.001292455941438675\n",
      "Batch: 136/158, Loss: 0.0013334566028788686\n",
      "Batch: 137/158, Loss: 0.0012512177927419543\n",
      "Batch: 138/158, Loss: 0.0014504602877423167\n",
      "Batch: 139/158, Loss: 0.001177717582322657\n",
      "Batch: 140/158, Loss: 0.0014220717130228877\n",
      "Batch: 141/158, Loss: 0.0013885771622881293\n",
      "Batch: 142/158, Loss: 0.0014249160885810852\n",
      "Batch: 143/158, Loss: 0.001023483695462346\n",
      "Batch: 144/158, Loss: 0.0013056842144578695\n",
      "Batch: 145/158, Loss: 0.0013358664000406861\n",
      "Batch: 146/158, Loss: 0.0010544430697336793\n",
      "Batch: 147/158, Loss: 0.0013959947973489761\n",
      "Batch: 148/158, Loss: 0.001346615725196898\n",
      "Batch: 149/158, Loss: 0.0013450991827994585\n",
      "Batch: 150/158, Loss: 0.0011276084696874022\n",
      "Batch: 151/158, Loss: 0.0014655705308541656\n",
      "Batch: 152/158, Loss: 0.0012602823553606868\n",
      "Batch: 153/158, Loss: 0.0012263641692698002\n",
      "Batch: 154/158, Loss: 0.0014072447083890438\n",
      "Batch: 155/158, Loss: 0.0012294447515159845\n",
      "Batch: 156/158, Loss: 0.0012025231262668967\n",
      "Batch: 157/158, Loss: 0.0012309156591072679\n",
      "Batch: 158/158, Loss: 0.0011645196937024593\n",
      "Epoch: 7/10, Loss: 0.22804902447387576, Time: 71.37267303466797\n",
      "Batch: 1/158, Loss: 0.0013570352457463741\n",
      "Batch: 2/158, Loss: 0.0013762179296463728\n",
      "Batch: 3/158, Loss: 0.001385093666613102\n",
      "Batch: 4/158, Loss: 0.001014232519082725\n",
      "Batch: 5/158, Loss: 0.000945442880038172\n",
      "Batch: 6/158, Loss: 0.0012108341325074434\n",
      "Batch: 7/158, Loss: 0.0013313080416992307\n",
      "Batch: 8/158, Loss: 0.000991046312265098\n",
      "Batch: 9/158, Loss: 0.0013833018019795418\n",
      "Batch: 10/158, Loss: 0.0013012462295591831\n",
      "Batch: 11/158, Loss: 0.0011963930446654558\n",
      "Batch: 12/158, Loss: 0.0011508380994200706\n",
      "Batch: 13/158, Loss: 0.0011902399128302932\n",
      "Batch: 14/158, Loss: 0.0012620091438293457\n",
      "Batch: 15/158, Loss: 0.0009260615333914757\n",
      "Batch: 16/158, Loss: 0.0012981636682525277\n",
      "Batch: 17/158, Loss: 0.001336286892183125\n",
      "Batch: 18/158, Loss: 0.001156010664999485\n",
      "Batch: 19/158, Loss: 0.0012203308288007975\n",
      "Batch: 20/158, Loss: 0.0009816946694627404\n",
      "Batch: 21/158, Loss: 0.0009238472557626665\n",
      "Batch: 22/158, Loss: 0.0014442867832258344\n",
      "Batch: 23/158, Loss: 0.0009370077750645578\n",
      "Batch: 24/158, Loss: 0.0010927051771432161\n",
      "Batch: 25/158, Loss: 0.0011347829131409526\n",
      "Batch: 26/158, Loss: 0.0012130294926464558\n",
      "Batch: 27/158, Loss: 0.0010919186752289534\n",
      "Batch: 28/158, Loss: 0.0014701224863529205\n",
      "Batch: 29/158, Loss: 0.0010128688300028443\n",
      "Batch: 30/158, Loss: 0.0010320405708625913\n",
      "Batch: 31/158, Loss: 0.0012337334919720888\n",
      "Batch: 32/158, Loss: 0.001425126101821661\n",
      "Batch: 33/158, Loss: 0.0011954469373449683\n",
      "Batch: 34/158, Loss: 0.0011949599720537663\n",
      "Batch: 35/158, Loss: 0.001156033482402563\n",
      "Batch: 36/158, Loss: 0.001006381236948073\n",
      "Batch: 37/158, Loss: 0.0012225810205563903\n",
      "Batch: 38/158, Loss: 0.0014033069601282477\n",
      "Batch: 39/158, Loss: 0.0011074896901845932\n",
      "Batch: 40/158, Loss: 0.0012773321941494942\n",
      "Batch: 41/158, Loss: 0.0009575840667821467\n",
      "Batch: 42/158, Loss: 0.0011912145419046283\n",
      "Batch: 43/158, Loss: 0.0011604512110352516\n",
      "Batch: 44/158, Loss: 0.0011755687883123755\n",
      "Batch: 45/158, Loss: 0.0012819324620068073\n",
      "Batch: 46/158, Loss: 0.0010453524300828576\n",
      "Batch: 47/158, Loss: 0.000889342452865094\n",
      "Batch: 48/158, Loss: 0.0013271598145365715\n",
      "Batch: 49/158, Loss: 0.001051597180776298\n",
      "Batch: 50/158, Loss: 0.001267339801415801\n",
      "Batch: 51/158, Loss: 0.001271023415029049\n",
      "Batch: 52/158, Loss: 0.0011773125734180212\n",
      "Batch: 53/158, Loss: 0.001064468757249415\n",
      "Batch: 54/158, Loss: 0.0009301731479354203\n",
      "Batch: 55/158, Loss: 0.001073368126526475\n",
      "Batch: 56/158, Loss: 0.0011535536032170057\n",
      "Batch: 57/158, Loss: 0.0011246012290939689\n",
      "Batch: 58/158, Loss: 0.0011355025926604867\n",
      "Batch: 59/158, Loss: 0.001030587824061513\n",
      "Batch: 60/158, Loss: 0.0009048892534337938\n",
      "Batch: 61/158, Loss: 0.001015009474940598\n",
      "Batch: 62/158, Loss: 0.0012412377400323749\n",
      "Batch: 63/158, Loss: 0.0010197072988376021\n",
      "Batch: 64/158, Loss: 0.0010663678403943777\n",
      "Batch: 65/158, Loss: 0.0008994041127152741\n",
      "Batch: 66/158, Loss: 0.000833844009321183\n",
      "Batch: 67/158, Loss: 0.001143826637417078\n",
      "Batch: 68/158, Loss: 0.0008672031108289957\n",
      "Batch: 69/158, Loss: 0.0009973103879019618\n",
      "Batch: 70/158, Loss: 0.001238259021192789\n",
      "Batch: 71/158, Loss: 0.0011340086348354816\n",
      "Batch: 72/158, Loss: 0.00114208715967834\n",
      "Batch: 73/158, Loss: 0.001209857640787959\n",
      "Batch: 74/158, Loss: 0.0013105026446282864\n",
      "Batch: 75/158, Loss: 0.0009725650306791067\n",
      "Batch: 76/158, Loss: 0.0010753427632153034\n",
      "Batch: 77/158, Loss: 0.0010976450284942985\n",
      "Batch: 78/158, Loss: 0.0008074704091995955\n",
      "Batch: 79/158, Loss: 0.0010812280233949423\n",
      "Batch: 80/158, Loss: 0.0011368944542482495\n",
      "Batch: 81/158, Loss: 0.0009130927501246333\n",
      "Batch: 82/158, Loss: 0.0011735691223293543\n",
      "Batch: 83/158, Loss: 0.0011179645080119371\n",
      "Batch: 84/158, Loss: 0.0009352947236038744\n",
      "Batch: 85/158, Loss: 0.0011443892726674676\n",
      "Batch: 86/158, Loss: 0.0011416005436331034\n",
      "Batch: 87/158, Loss: 0.000990470638498664\n",
      "Batch: 88/158, Loss: 0.0010006465017795563\n",
      "Batch: 89/158, Loss: 0.0010781522141769528\n",
      "Batch: 90/158, Loss: 0.0012201651697978377\n",
      "Batch: 91/158, Loss: 0.0011959318071603775\n",
      "Batch: 92/158, Loss: 0.00124367605894804\n",
      "Batch: 93/158, Loss: 0.0010646599112078547\n",
      "Batch: 94/158, Loss: 0.0011274036951363087\n",
      "Batch: 95/158, Loss: 0.0010719847632572055\n",
      "Batch: 96/158, Loss: 0.0010459424229338765\n",
      "Batch: 97/158, Loss: 0.0010093551827594638\n",
      "Batch: 98/158, Loss: 0.0009896932169795036\n",
      "Batch: 99/158, Loss: 0.0011210470693185925\n",
      "Batch: 100/158, Loss: 0.0009925089543685317\n",
      "Batch: 101/158, Loss: 0.0010257773101329803\n",
      "Batch: 102/158, Loss: 0.0010288502089679241\n",
      "Batch: 103/158, Loss: 0.0011899832170456648\n",
      "Batch: 104/158, Loss: 0.001060260459780693\n",
      "Batch: 105/158, Loss: 0.0010987838031724095\n",
      "Batch: 106/158, Loss: 0.0009173968574032187\n",
      "Batch: 107/158, Loss: 0.0008002378745004535\n",
      "Batch: 108/158, Loss: 0.001133318874053657\n",
      "Batch: 109/158, Loss: 0.0010048593394458294\n",
      "Batch: 110/158, Loss: 0.0010101102525368333\n",
      "Batch: 111/158, Loss: 0.0009964063065126538\n",
      "Batch: 112/158, Loss: 0.0010859197936952114\n",
      "Batch: 113/158, Loss: 0.0011638777796179056\n",
      "Batch: 114/158, Loss: 0.000993445748463273\n",
      "Batch: 115/158, Loss: 0.0012782433768734336\n",
      "Batch: 116/158, Loss: 0.0009739198139868677\n",
      "Batch: 117/158, Loss: 0.0011688251979649067\n",
      "Batch: 118/158, Loss: 0.0009183111251331866\n",
      "Batch: 119/158, Loss: 0.0009932133834809065\n",
      "Batch: 120/158, Loss: 0.0011894756462424994\n",
      "Batch: 121/158, Loss: 0.001136211445555091\n",
      "Batch: 122/158, Loss: 0.0008494233479723334\n",
      "Batch: 123/158, Loss: 0.0009776169899851084\n",
      "Batch: 124/158, Loss: 0.0009166939998976886\n",
      "Batch: 125/158, Loss: 0.0011425609700381756\n",
      "Batch: 126/158, Loss: 0.0008486988372169435\n",
      "Batch: 127/158, Loss: 0.0010285531170666218\n",
      "Batch: 128/158, Loss: 0.0008213778492063284\n",
      "Batch: 129/158, Loss: 0.0010918814223259687\n",
      "Batch: 130/158, Loss: 0.0011206298368051648\n",
      "Batch: 131/158, Loss: 0.0011403667740523815\n",
      "Batch: 132/158, Loss: 0.0009890436194837093\n",
      "Batch: 133/158, Loss: 0.001064860145561397\n",
      "Batch: 134/158, Loss: 0.0010759414872154593\n",
      "Batch: 135/158, Loss: 0.0011554535012692213\n",
      "Batch: 136/158, Loss: 0.0010153799084946513\n",
      "Batch: 137/158, Loss: 0.0009518403676338494\n",
      "Batch: 138/158, Loss: 0.0009053954272530973\n",
      "Batch: 139/158, Loss: 0.0012691360898315907\n",
      "Batch: 140/158, Loss: 0.0008916042861528695\n",
      "Batch: 141/158, Loss: 0.0008082762942649424\n",
      "Batch: 142/158, Loss: 0.0011386674595996737\n",
      "Batch: 143/158, Loss: 0.0009017414413392544\n",
      "Batch: 144/158, Loss: 0.001221408136188984\n",
      "Batch: 145/158, Loss: 0.000939887308049947\n",
      "Batch: 146/158, Loss: 0.0010382779873907566\n",
      "Batch: 147/158, Loss: 0.0009371936321258545\n",
      "Batch: 148/158, Loss: 0.000929575297050178\n",
      "Batch: 149/158, Loss: 0.001193777658045292\n",
      "Batch: 150/158, Loss: 0.0009383008000440896\n",
      "Batch: 151/158, Loss: 0.001135307364165783\n",
      "Batch: 152/158, Loss: 0.0011165302712470293\n",
      "Batch: 153/158, Loss: 0.0008625162299722433\n",
      "Batch: 154/158, Loss: 0.0010446208762004972\n",
      "Batch: 155/158, Loss: 0.0010046304669231176\n",
      "Batch: 156/158, Loss: 0.0011049382155761123\n",
      "Batch: 157/158, Loss: 0.0009747911244630814\n",
      "Batch: 158/158, Loss: 0.0012733557960018516\n",
      "Epoch: 8/10, Loss: 0.17265681363642216, Time: 72.32281422615051\n",
      "Batch: 1/158, Loss: 0.0009897578274831176\n",
      "Batch: 2/158, Loss: 0.0008720639743842185\n",
      "Batch: 3/158, Loss: 0.0009921619202941656\n",
      "Batch: 4/158, Loss: 0.0010036908788606524\n",
      "Batch: 5/158, Loss: 0.0008828333811834455\n",
      "Batch: 6/158, Loss: 0.0010913887526839972\n",
      "Batch: 7/158, Loss: 0.0008972061914391816\n",
      "Batch: 8/158, Loss: 0.0008923868299461901\n",
      "Batch: 9/158, Loss: 0.0010233293287456036\n",
      "Batch: 10/158, Loss: 0.0007860863697715104\n",
      "Batch: 11/158, Loss: 0.0008007984724827111\n",
      "Batch: 12/158, Loss: 0.0009448895580135286\n",
      "Batch: 13/158, Loss: 0.0008477095398120582\n",
      "Batch: 14/158, Loss: 0.0008305902010761201\n",
      "Batch: 15/158, Loss: 0.000907916750293225\n",
      "Batch: 16/158, Loss: 0.000939583289436996\n",
      "Batch: 17/158, Loss: 0.0011610595975071192\n",
      "Batch: 18/158, Loss: 0.000974312424659729\n",
      "Batch: 19/158, Loss: 0.0009485467453487217\n",
      "Batch: 20/158, Loss: 0.0009013481321744621\n",
      "Batch: 21/158, Loss: 0.0007699559791944921\n",
      "Batch: 22/158, Loss: 0.0010469447588548064\n",
      "Batch: 23/158, Loss: 0.0008580147987231612\n",
      "Batch: 24/158, Loss: 0.000900966755580157\n",
      "Batch: 25/158, Loss: 0.0007051245775073767\n",
      "Batch: 26/158, Loss: 0.0009436944383196533\n",
      "Batch: 27/158, Loss: 0.0008083437569439411\n",
      "Batch: 28/158, Loss: 0.0009472443489357829\n",
      "Batch: 29/158, Loss: 0.0010388313094154\n",
      "Batch: 30/158, Loss: 0.0009355490910820663\n",
      "Batch: 31/158, Loss: 0.000994281261228025\n",
      "Batch: 32/158, Loss: 0.0007599079399369657\n",
      "Batch: 33/158, Loss: 0.0008480322430841625\n",
      "Batch: 34/158, Loss: 0.0008205928606912494\n",
      "Batch: 35/158, Loss: 0.0008401762461289763\n",
      "Batch: 36/158, Loss: 0.000953206792473793\n",
      "Batch: 37/158, Loss: 0.0006746361614204943\n",
      "Batch: 38/158, Loss: 0.0007366979261860251\n",
      "Batch: 39/158, Loss: 0.0010722600854933262\n",
      "Batch: 40/158, Loss: 0.000821910216473043\n",
      "Batch: 41/158, Loss: 0.0010422869818285108\n",
      "Batch: 42/158, Loss: 0.0008810233557596803\n",
      "Batch: 43/158, Loss: 0.0007817796431481838\n",
      "Batch: 44/158, Loss: 0.0007971034501679242\n",
      "Batch: 45/158, Loss: 0.0009278951911255717\n",
      "Batch: 46/158, Loss: 0.0007820054888725281\n",
      "Batch: 47/158, Loss: 0.0009765720460563898\n",
      "Batch: 48/158, Loss: 0.0008760532364249229\n",
      "Batch: 49/158, Loss: 0.0010272860527038574\n",
      "Batch: 50/158, Loss: 0.001130470191128552\n",
      "Batch: 51/158, Loss: 0.0008319205953739583\n",
      "Batch: 52/158, Loss: 0.0008765587699599564\n",
      "Batch: 53/158, Loss: 0.0009353366331197321\n",
      "Batch: 54/158, Loss: 0.0009261674131266773\n",
      "Batch: 55/158, Loss: 0.0007874929578974843\n",
      "Batch: 56/158, Loss: 0.0008701145416125655\n",
      "Batch: 57/158, Loss: 0.0009307856671512127\n",
      "Batch: 58/158, Loss: 0.0008169596549123526\n",
      "Batch: 59/158, Loss: 0.0008831541053950787\n",
      "Batch: 60/158, Loss: 0.0009461174486204982\n",
      "Batch: 61/158, Loss: 0.0008633758407086134\n",
      "Batch: 62/158, Loss: 0.0007973322062753141\n",
      "Batch: 63/158, Loss: 0.0009497033315710723\n",
      "Batch: 64/158, Loss: 0.0008618729189038277\n",
      "Batch: 65/158, Loss: 0.0008872770122252405\n",
      "Batch: 66/158, Loss: 0.0009057553834281862\n",
      "Batch: 67/158, Loss: 0.0008746889652684331\n",
      "Batch: 68/158, Loss: 0.0008638242143206298\n",
      "Batch: 69/158, Loss: 0.0007937715272419155\n",
      "Batch: 70/158, Loss: 0.0008168331114575267\n",
      "Batch: 71/158, Loss: 0.0007560881203971803\n",
      "Batch: 72/158, Loss: 0.001050271326676011\n",
      "Batch: 73/158, Loss: 0.0008366800611838698\n",
      "Batch: 74/158, Loss: 0.0009690253064036369\n",
      "Batch: 75/158, Loss: 0.0009704672847874463\n",
      "Batch: 76/158, Loss: 0.0008909080061130226\n",
      "Batch: 77/158, Loss: 0.0008604045724496245\n",
      "Batch: 78/158, Loss: 0.0008991708164103329\n",
      "Batch: 79/158, Loss: 0.0009265345870517194\n",
      "Batch: 80/158, Loss: 0.0009171757264994085\n",
      "Batch: 81/158, Loss: 0.0008704949868842959\n",
      "Batch: 82/158, Loss: 0.0008798955241218209\n",
      "Batch: 83/158, Loss: 0.0009611867135390639\n",
      "Batch: 84/158, Loss: 0.0009196899482049048\n",
      "Batch: 85/158, Loss: 0.000777625129558146\n",
      "Batch: 86/158, Loss: 0.000910285220015794\n",
      "Batch: 87/158, Loss: 0.0008266124641522765\n",
      "Batch: 88/158, Loss: 0.0006924057961441576\n",
      "Batch: 89/158, Loss: 0.0009935024427250028\n",
      "Batch: 90/158, Loss: 0.0010604368289932609\n",
      "Batch: 91/158, Loss: 0.0009341993718408048\n",
      "Batch: 92/158, Loss: 0.0007444150978699327\n",
      "Batch: 93/158, Loss: 0.0008506547310389578\n",
      "Batch: 94/158, Loss: 0.0007783208857290447\n",
      "Batch: 95/158, Loss: 0.0008716523880138993\n",
      "Batch: 96/158, Loss: 0.000953814247623086\n",
      "Batch: 97/158, Loss: 0.0008353900630027056\n",
      "Batch: 98/158, Loss: 0.0009589254623278975\n",
      "Batch: 99/158, Loss: 0.000718413561116904\n",
      "Batch: 100/158, Loss: 0.000846143695525825\n",
      "Batch: 101/158, Loss: 0.0007353734690696001\n",
      "Batch: 102/158, Loss: 0.0008237962028943002\n",
      "Batch: 103/158, Loss: 0.0009970184182748199\n",
      "Batch: 104/158, Loss: 0.0007411072147078812\n",
      "Batch: 105/158, Loss: 0.0008093351498246193\n",
      "Batch: 106/158, Loss: 0.0009334792848676443\n",
      "Batch: 107/158, Loss: 0.0008418397046625614\n",
      "Batch: 108/158, Loss: 0.0007855293806642294\n",
      "Batch: 109/158, Loss: 0.0005907322629354894\n",
      "Batch: 110/158, Loss: 0.0008106936002150178\n",
      "Batch: 111/158, Loss: 0.0007822969346307218\n",
      "Batch: 112/158, Loss: 0.0008351629949174821\n",
      "Batch: 113/158, Loss: 0.0007467776304110885\n",
      "Batch: 114/158, Loss: 0.0008606067858636379\n",
      "Batch: 115/158, Loss: 0.0007443006034009159\n",
      "Batch: 116/158, Loss: 0.0007842642371542752\n",
      "Batch: 117/158, Loss: 0.0008259593741968274\n",
      "Batch: 118/158, Loss: 0.0008190862718038261\n",
      "Batch: 119/158, Loss: 0.0005994381499476731\n",
      "Batch: 120/158, Loss: 0.0007177921361289918\n",
      "Batch: 121/158, Loss: 0.0008677885052748024\n",
      "Batch: 122/158, Loss: 0.0008548258920200169\n",
      "Batch: 123/158, Loss: 0.0005529650952666998\n",
      "Batch: 124/158, Loss: 0.0007170417811721563\n",
      "Batch: 125/158, Loss: 0.0007926264661364257\n",
      "Batch: 126/158, Loss: 0.0007412925479002297\n",
      "Batch: 127/158, Loss: 0.0008746220846660435\n",
      "Batch: 128/158, Loss: 0.0008759983466006815\n",
      "Batch: 129/158, Loss: 0.0008119693957269192\n",
      "Batch: 130/158, Loss: 0.0008697229204699397\n",
      "Batch: 131/158, Loss: 0.0008713458082638681\n",
      "Batch: 132/158, Loss: 0.000825441034976393\n",
      "Batch: 133/158, Loss: 0.0007153295446187258\n",
      "Batch: 134/158, Loss: 0.0008456972427666187\n",
      "Batch: 135/158, Loss: 0.0008693857816979289\n",
      "Batch: 136/158, Loss: 0.0008019909728318453\n",
      "Batch: 137/158, Loss: 0.0007113575702533126\n",
      "Batch: 138/158, Loss: 0.0006084109772928059\n",
      "Batch: 139/158, Loss: 0.000846545968670398\n",
      "Batch: 140/158, Loss: 0.0008021575049497187\n",
      "Batch: 141/158, Loss: 0.0008168711210601032\n",
      "Batch: 142/158, Loss: 0.0008075282676145434\n",
      "Batch: 143/158, Loss: 0.0008222952019423246\n",
      "Batch: 144/158, Loss: 0.0007294775568880141\n",
      "Batch: 145/158, Loss: 0.000624058477114886\n",
      "Batch: 146/158, Loss: 0.000824112503323704\n",
      "Batch: 147/158, Loss: 0.0009058057912625372\n",
      "Batch: 148/158, Loss: 0.0008474261267110705\n",
      "Batch: 149/158, Loss: 0.0009183224756270647\n",
      "Batch: 150/158, Loss: 0.0006862333393655717\n",
      "Batch: 151/158, Loss: 0.0008308014366775751\n",
      "Batch: 152/158, Loss: 0.0007419022149406374\n",
      "Batch: 153/158, Loss: 0.000872837204951793\n",
      "Batch: 154/158, Loss: 0.0006732373149134219\n",
      "Batch: 155/158, Loss: 0.0009428916382603347\n",
      "Batch: 156/158, Loss: 0.0008533257059752941\n",
      "Batch: 157/158, Loss: 0.0008544562151655555\n",
      "Batch: 158/158, Loss: 0.000915238750167191\n",
      "Epoch: 9/10, Loss: 0.13557426660554484, Time: 71.3712649345398\n",
      "Batch: 1/158, Loss: 0.000737389549612999\n",
      "Batch: 2/158, Loss: 0.0007667269092053175\n",
      "Batch: 3/158, Loss: 0.0007889687549322844\n",
      "Batch: 4/158, Loss: 0.0007473293226212263\n",
      "Batch: 5/158, Loss: 0.0007180863758549094\n",
      "Batch: 6/158, Loss: 0.0006698320503346622\n",
      "Batch: 7/158, Loss: 0.0008866311400197446\n",
      "Batch: 8/158, Loss: 0.0007256163517013192\n",
      "Batch: 9/158, Loss: 0.0008176431292667985\n",
      "Batch: 10/158, Loss: 0.0005613003741018474\n",
      "Batch: 11/158, Loss: 0.0005875854985788465\n",
      "Batch: 12/158, Loss: 0.0005617701099254191\n",
      "Batch: 13/158, Loss: 0.0006565157091245055\n",
      "Batch: 14/158, Loss: 0.0007785111083649099\n",
      "Batch: 15/158, Loss: 0.0007833359995856881\n",
      "Batch: 16/158, Loss: 0.0007503109518438578\n",
      "Batch: 17/158, Loss: 0.0005611235974356532\n",
      "Batch: 18/158, Loss: 0.0006860147113911808\n",
      "Batch: 19/158, Loss: 0.0008162715821526945\n",
      "Batch: 20/158, Loss: 0.0007766359485685825\n",
      "Batch: 21/158, Loss: 0.0006933848490007222\n",
      "Batch: 22/158, Loss: 0.0006960491882637143\n",
      "Batch: 23/158, Loss: 0.0006798786926083267\n",
      "Batch: 24/158, Loss: 0.0006495045381598175\n",
      "Batch: 25/158, Loss: 0.0006407145410776138\n",
      "Batch: 26/158, Loss: 0.0006786876474507153\n",
      "Batch: 27/158, Loss: 0.0006417650729417801\n",
      "Batch: 28/158, Loss: 0.0007639100076630712\n",
      "Batch: 29/158, Loss: 0.000682901474647224\n",
      "Batch: 30/158, Loss: 0.0006699000368826091\n",
      "Batch: 31/158, Loss: 0.0007774480036459863\n",
      "Batch: 32/158, Loss: 0.0008075472433120012\n",
      "Batch: 33/158, Loss: 0.000645314110442996\n",
      "Batch: 34/158, Loss: 0.0007408407400362194\n",
      "Batch: 35/158, Loss: 0.0006753344787284732\n",
      "Batch: 36/158, Loss: 0.0008626305498182774\n",
      "Batch: 37/158, Loss: 0.0006996880983933806\n",
      "Batch: 38/158, Loss: 0.0007319963769987226\n",
      "Batch: 39/158, Loss: 0.000762774667236954\n",
      "Batch: 40/158, Loss: 0.0007229901966638863\n",
      "Batch: 41/158, Loss: 0.0006582126952707767\n",
      "Batch: 42/158, Loss: 0.0007457411265932024\n",
      "Batch: 43/158, Loss: 0.0006305749993771315\n",
      "Batch: 44/158, Loss: 0.0006108944653533399\n",
      "Batch: 45/158, Loss: 0.0007669311598874629\n",
      "Batch: 46/158, Loss: 0.0006263075047172606\n",
      "Batch: 47/158, Loss: 0.0006413227529264987\n",
      "Batch: 48/158, Loss: 0.0008426955319009721\n",
      "Batch: 49/158, Loss: 0.0007673880318179727\n",
      "Batch: 50/158, Loss: 0.0008560499991290271\n",
      "Batch: 51/158, Loss: 0.0008148322231136262\n",
      "Batch: 52/158, Loss: 0.0007165975985117257\n",
      "Batch: 53/158, Loss: 0.0007276948308572173\n",
      "Batch: 54/158, Loss: 0.000834476260934025\n",
      "Batch: 55/158, Loss: 0.0008159723365679383\n",
      "Batch: 56/158, Loss: 0.0007854475406929851\n",
      "Batch: 57/158, Loss: 0.0007231266354210675\n",
      "Batch: 58/158, Loss: 0.0007379121379926801\n",
      "Batch: 59/158, Loss: 0.0007886902312748134\n",
      "Batch: 60/158, Loss: 0.0007988313445821404\n",
      "Batch: 61/158, Loss: 0.0006655582110397518\n",
      "Batch: 62/158, Loss: 0.0008689279784448445\n",
      "Batch: 63/158, Loss: 0.000578699167817831\n",
      "Batch: 64/158, Loss: 0.0006897385464981198\n",
      "Batch: 65/158, Loss: 0.0006583674112334847\n",
      "Batch: 66/158, Loss: 0.0008123925072140992\n",
      "Batch: 67/158, Loss: 0.0007944466196931899\n",
      "Batch: 68/158, Loss: 0.0006083837361074984\n",
      "Batch: 69/158, Loss: 0.0006207736441865563\n",
      "Batch: 70/158, Loss: 0.0007173380581662059\n",
      "Batch: 71/158, Loss: 0.0006420555873773992\n",
      "Batch: 72/158, Loss: 0.0006952810799703002\n",
      "Batch: 73/158, Loss: 0.0007932748412713408\n",
      "Batch: 74/158, Loss: 0.000762564071919769\n",
      "Batch: 75/158, Loss: 0.0006439793505705893\n",
      "Batch: 76/158, Loss: 0.0007336419075727463\n",
      "Batch: 77/158, Loss: 0.0006839492125436664\n",
      "Batch: 78/158, Loss: 0.0006845968891866505\n",
      "Batch: 79/158, Loss: 0.0006880570435896516\n",
      "Batch: 80/158, Loss: 0.0007268763147294521\n",
      "Batch: 81/158, Loss: 0.0007089822902344167\n",
      "Batch: 82/158, Loss: 0.0006841407157480717\n",
      "Batch: 83/158, Loss: 0.0006805779412388802\n",
      "Batch: 84/158, Loss: 0.0007739166030660272\n",
      "Batch: 85/158, Loss: 0.0007094545871950686\n",
      "Batch: 86/158, Loss: 0.0007839905447326601\n",
      "Batch: 87/158, Loss: 0.000669797183945775\n",
      "Batch: 88/158, Loss: 0.0006954606506042182\n",
      "Batch: 89/158, Loss: 0.0007424283539876342\n",
      "Batch: 90/158, Loss: 0.0006177396280691028\n",
      "Batch: 91/158, Loss: 0.0006513251573778689\n",
      "Batch: 92/158, Loss: 0.0006797917885705829\n",
      "Batch: 93/158, Loss: 0.0006587096722796559\n",
      "Batch: 94/158, Loss: 0.0006988552049733698\n",
      "Batch: 95/158, Loss: 0.000662283506244421\n",
      "Batch: 96/158, Loss: 0.0006752399494871497\n",
      "Batch: 97/158, Loss: 0.0006669881986454129\n",
      "Batch: 98/158, Loss: 0.0006205023964866996\n",
      "Batch: 99/158, Loss: 0.0006248350837267935\n",
      "Batch: 100/158, Loss: 0.0005963364965282381\n",
      "Batch: 101/158, Loss: 0.0005382313975133002\n",
      "Batch: 102/158, Loss: 0.0007419646717607975\n",
      "Batch: 103/158, Loss: 0.0005744299269281328\n",
      "Batch: 104/158, Loss: 0.0006390533526428044\n",
      "Batch: 105/158, Loss: 0.0006004682509228587\n",
      "Batch: 106/158, Loss: 0.0005354089080356061\n",
      "Batch: 107/158, Loss: 0.0006139010656625032\n",
      "Batch: 108/158, Loss: 0.0007214790675789118\n",
      "Batch: 109/158, Loss: 0.0005326502141542733\n",
      "Batch: 110/158, Loss: 0.000617843703366816\n",
      "Batch: 111/158, Loss: 0.0005805691471323371\n",
      "Batch: 112/158, Loss: 0.0007530648726969957\n",
      "Batch: 113/158, Loss: 0.0005917392554692924\n",
      "Batch: 114/158, Loss: 0.0007071454310789704\n",
      "Batch: 115/158, Loss: 0.0005363053642213345\n",
      "Batch: 116/158, Loss: 0.0006161080673336983\n",
      "Batch: 117/158, Loss: 0.0007033291622065008\n",
      "Batch: 118/158, Loss: 0.0006871288642287254\n",
      "Batch: 119/158, Loss: 0.0006380134727805853\n",
      "Batch: 120/158, Loss: 0.0007200922118499875\n",
      "Batch: 121/158, Loss: 0.000689953682012856\n",
      "Batch: 122/158, Loss: 0.000555808306671679\n",
      "Batch: 123/158, Loss: 0.0005454191123135388\n",
      "Batch: 124/158, Loss: 0.0008353516459465027\n",
      "Batch: 125/158, Loss: 0.0005998261040076613\n",
      "Batch: 126/158, Loss: 0.000606535526458174\n",
      "Batch: 127/158, Loss: 0.0005702323978766799\n",
      "Batch: 128/158, Loss: 0.0006307793082669377\n",
      "Batch: 129/158, Loss: 0.0006182030192576349\n",
      "Batch: 130/158, Loss: 0.0006794430664740503\n",
      "Batch: 131/158, Loss: 0.000634362397249788\n",
      "Batch: 132/158, Loss: 0.0006049289368093014\n",
      "Batch: 133/158, Loss: 0.0006737541989423335\n",
      "Batch: 134/158, Loss: 0.0006867271149531007\n",
      "Batch: 135/158, Loss: 0.0007192908669821918\n",
      "Batch: 136/158, Loss: 0.0006559062749147415\n",
      "Batch: 137/158, Loss: 0.0007032149005681276\n",
      "Batch: 138/158, Loss: 0.0006229550926946104\n",
      "Batch: 139/158, Loss: 0.0006227898993529379\n",
      "Batch: 140/158, Loss: 0.0006499650189653039\n",
      "Batch: 141/158, Loss: 0.000631761213298887\n",
      "Batch: 142/158, Loss: 0.0006437973934225738\n",
      "Batch: 143/158, Loss: 0.0006328243180178106\n",
      "Batch: 144/158, Loss: 0.0005854563787579536\n",
      "Batch: 145/158, Loss: 0.000666145933791995\n",
      "Batch: 146/158, Loss: 0.0006907928618602455\n",
      "Batch: 147/158, Loss: 0.0006879288703203201\n",
      "Batch: 148/158, Loss: 0.0006541007896885276\n",
      "Batch: 149/158, Loss: 0.0006762819248251617\n",
      "Batch: 150/158, Loss: 0.0006891376106068492\n",
      "Batch: 151/158, Loss: 0.0006874924874864519\n",
      "Batch: 152/158, Loss: 0.0005927488091401756\n",
      "Batch: 153/158, Loss: 0.0006938781007193029\n",
      "Batch: 154/158, Loss: 0.0006152049754746258\n",
      "Batch: 155/158, Loss: 0.0005380621878430247\n",
      "Batch: 156/158, Loss: 0.0006534437416121364\n",
      "Batch: 157/158, Loss: 0.0006859214627183974\n",
      "Batch: 158/158, Loss: 0.0007976564229466021\n",
      "Epoch: 10/10, Loss: 0.10853782598860562, Time: 70.37015724182129\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "!mkdir -p checkpoints/llm\n",
    "train_history = {}\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    train_history[epoch] = []\n",
    "    for i, (english, korean) in enumerate(dataloader):\n",
    "        english, korean = english.to(device), korean.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(english, korean)\n",
    "        output = output.view(-1, output.shape[2])\n",
    "        korean = korean.view(-1)\n",
    "        loss = criterion(output, korean)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Batch: {i+1}/{len(dataloader)}, Loss: {loss.item()}\")\n",
    "        train_history[epoch].append(loss.item())\n",
    "    end = time.time()\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Loss: {total_loss}, Time: {end-start}\")\n",
    "    # save checkpoint\n",
    "    torch.save(model.state_dict(), f\"checkpoints/llm/epoch_{epoch+1}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:32:55.449436Z",
     "start_time": "2025-03-14T06:20:54.340825Z"
    }
   },
   "id": "185c159af4f6cc2e"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the training history\n",
    "with open(\"checkpoints/llm/train_history.json\", \"w\") as file:\n",
    "    json.dump(train_history, file)\n",
    "    \n",
    "# Plot the training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "epoch_loss = []\n",
    "for epoch, loss in train_history.items():\n",
    "    # sum the loss of each batch\n",
    "    epoch_loss.append(sum(loss))\n",
    "running_batch_loss = []\n",
    "for epoch, loss in train_history.items():\n",
    "    running_batch_loss.extend(loss)\n",
    "\n",
    "# plot sum of epoch loss (or do we do average? Can one of you confirm?)\n",
    "plt.plot(epoch_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "# plot running batch loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(running_batch_loss)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:38:37.490414Z",
     "start_time": "2025-03-14T06:38:37.362520Z"
    }
   },
   "id": "9806a79d964d3a02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13b38bc23d029985"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1/151, Loss: 0.00047055541654117405\n",
      "Batch: 2/151, Loss: 0.0006839927518740296\n",
      "Batch: 3/151, Loss: 0.0006065939087420702\n",
      "Batch: 4/151, Loss: 0.007105179131031036\n",
      "Batch: 5/151, Loss: 0.0006620482308790088\n",
      "Batch: 6/151, Loss: 0.00075148930773139\n",
      "Batch: 7/151, Loss: 0.0006550325197167695\n",
      "Batch: 8/151, Loss: 0.0005484517896547914\n",
      "Batch: 9/151, Loss: 0.000728233193513006\n",
      "Batch: 10/151, Loss: 0.0005002414691261947\n",
      "Batch: 11/151, Loss: 0.0005790849099867046\n",
      "Batch: 12/151, Loss: 0.0005756817990913987\n",
      "Batch: 13/151, Loss: 0.0005467485752888024\n",
      "Batch: 14/151, Loss: 0.0006934659904800355\n",
      "Batch: 15/151, Loss: 0.0005279876058921218\n",
      "Batch: 16/151, Loss: 0.0006514612468890846\n",
      "Batch: 17/151, Loss: 0.0009132206905633211\n",
      "Batch: 18/151, Loss: 0.00045256136218085885\n",
      "Batch: 19/151, Loss: 0.0009643862722441554\n",
      "Batch: 20/151, Loss: 0.0008420009980909526\n",
      "Batch: 21/151, Loss: 0.0007838006713427603\n",
      "Batch: 22/151, Loss: 0.000751386396586895\n",
      "Batch: 23/151, Loss: 0.0045995269902050495\n",
      "Batch: 24/151, Loss: 0.0006754551432095468\n",
      "Batch: 25/151, Loss: 0.006500516086816788\n",
      "Batch: 26/151, Loss: 0.0005743654910475016\n",
      "Batch: 27/151, Loss: 0.0005413613398559391\n",
      "Batch: 28/151, Loss: 0.0008037291700020432\n",
      "Batch: 29/151, Loss: 0.0005012028268538415\n",
      "Batch: 30/151, Loss: 0.006974559277296066\n",
      "Batch: 31/151, Loss: 0.0007135199848562479\n",
      "Batch: 32/151, Loss: 0.0006485923076979816\n",
      "Batch: 33/151, Loss: 0.0006660044309683144\n",
      "Batch: 34/151, Loss: 0.0007135595078580081\n",
      "Batch: 35/151, Loss: 0.0008116592653095722\n",
      "Batch: 36/151, Loss: 0.006865311414003372\n",
      "Batch: 37/151, Loss: 0.0005630404339171946\n",
      "Batch: 38/151, Loss: 0.000699222378898412\n",
      "Batch: 39/151, Loss: 0.0006168770487420261\n",
      "Batch: 40/151, Loss: 0.0006230922299437225\n",
      "Batch: 41/151, Loss: 0.0007006547530181706\n",
      "Batch: 42/151, Loss: 0.0005580353899858892\n",
      "Batch: 43/151, Loss: 0.0007257283432409167\n",
      "Batch: 44/151, Loss: 0.0007164152339100838\n",
      "Batch: 45/151, Loss: 0.0007674847729504108\n",
      "Batch: 46/151, Loss: 0.0007349625811912119\n",
      "Batch: 47/151, Loss: 0.0008067530579864979\n",
      "Batch: 48/151, Loss: 0.0006178063922561705\n",
      "Batch: 49/151, Loss: 0.0005682333721779287\n",
      "Batch: 50/151, Loss: 0.000606417772360146\n",
      "Batch: 51/151, Loss: 0.0008427907596342266\n",
      "Batch: 52/151, Loss: 0.0007856603479012847\n",
      "Batch: 53/151, Loss: 0.0006585299852304161\n",
      "Batch: 54/151, Loss: 0.0007769693620502949\n",
      "Batch: 55/151, Loss: 0.0005977071123197675\n",
      "Batch: 56/151, Loss: 0.0004954690812155604\n",
      "Batch: 57/151, Loss: 0.0006369868060573936\n",
      "Batch: 58/151, Loss: 0.0007023724028840661\n",
      "Batch: 59/151, Loss: 0.0005893615889362991\n",
      "Batch: 60/151, Loss: 0.0005408661090768874\n",
      "Batch: 61/151, Loss: 0.0007134244660846889\n",
      "Batch: 62/151, Loss: 0.0005780772189609706\n",
      "Batch: 63/151, Loss: 0.0006505557103082538\n",
      "Batch: 64/151, Loss: 0.0005803150706924498\n",
      "Batch: 65/151, Loss: 0.0005381079390645027\n",
      "Batch: 66/151, Loss: 0.0006355544319376349\n",
      "Batch: 67/151, Loss: 0.0005947828758507967\n",
      "Batch: 68/151, Loss: 0.0007034050649963319\n",
      "Batch: 69/151, Loss: 0.0006817093817517161\n",
      "Batch: 70/151, Loss: 0.0006283172406256199\n",
      "Batch: 71/151, Loss: 0.0006340155960060656\n",
      "Batch: 72/151, Loss: 0.0005431676399894059\n",
      "Batch: 73/151, Loss: 0.0008520937408320606\n",
      "Batch: 74/151, Loss: 0.0008414761396124959\n",
      "Batch: 75/151, Loss: 0.0006827123579569161\n",
      "Batch: 76/151, Loss: 0.0005798537167720497\n",
      "Batch: 77/151, Loss: 0.000663036189507693\n",
      "Batch: 78/151, Loss: 0.0006334654171951115\n",
      "Batch: 79/151, Loss: 0.000630844384431839\n",
      "Batch: 80/151, Loss: 0.0006421285215765238\n",
      "Batch: 81/151, Loss: 0.0007384370546787977\n",
      "Batch: 82/151, Loss: 0.0007617064984515309\n",
      "Batch: 83/151, Loss: 0.0006006941548548639\n",
      "Batch: 84/151, Loss: 0.0007548769353888929\n",
      "Batch: 85/151, Loss: 0.0007209836039692163\n",
      "Batch: 86/151, Loss: 0.0008105147280730307\n",
      "Batch: 87/151, Loss: 0.0008742770878598094\n",
      "Batch: 88/151, Loss: 0.0006162020727060735\n",
      "Batch: 89/151, Loss: 0.0006724321283400059\n",
      "Batch: 90/151, Loss: 0.0005699103348888457\n",
      "Batch: 91/151, Loss: 0.0006730743334628642\n",
      "Batch: 92/151, Loss: 0.0008421214297413826\n",
      "Batch: 93/151, Loss: 0.0006497108843177557\n",
      "Batch: 94/151, Loss: 0.000696909730322659\n",
      "Batch: 95/151, Loss: 0.0006926795467734337\n",
      "Batch: 96/151, Loss: 0.0009345970465801656\n",
      "Batch: 97/151, Loss: 0.0006313332123681903\n",
      "Batch: 98/151, Loss: 0.0007044813246466219\n",
      "Batch: 99/151, Loss: 0.0007944206590764225\n",
      "Batch: 100/151, Loss: 0.0007834538700990379\n",
      "Batch: 101/151, Loss: 0.0009195011807605624\n",
      "Batch: 102/151, Loss: 0.0006491865497082472\n",
      "Batch: 103/151, Loss: 0.0005863341502845287\n",
      "Batch: 104/151, Loss: 0.0006803292781114578\n",
      "Batch: 105/151, Loss: 0.0008247886435128748\n",
      "Batch: 106/151, Loss: 0.0007353553082793951\n",
      "Batch: 107/151, Loss: 0.0008337110048159957\n",
      "Batch: 108/151, Loss: 0.000692794332280755\n",
      "Batch: 109/151, Loss: 0.0007249907939694822\n",
      "Batch: 110/151, Loss: 0.0005955998785793781\n",
      "Batch: 111/151, Loss: 0.0006799363764002919\n",
      "Batch: 112/151, Loss: 0.0006220466457307339\n",
      "Batch: 113/151, Loss: 0.0006416162941604853\n",
      "Batch: 114/151, Loss: 0.0007048072875477374\n",
      "Batch: 115/151, Loss: 0.0005535612581297755\n",
      "Batch: 116/151, Loss: 0.000689330801833421\n",
      "Batch: 117/151, Loss: 0.0010459691984578967\n",
      "Batch: 118/151, Loss: 0.0007424270152114332\n",
      "Batch: 119/151, Loss: 0.0008235250134021044\n",
      "Batch: 120/151, Loss: 0.000549078918993473\n",
      "Batch: 121/151, Loss: 0.0008467476582154632\n",
      "Batch: 122/151, Loss: 0.0004985527484677732\n",
      "Batch: 123/151, Loss: 0.0007383599295280874\n",
      "Batch: 124/151, Loss: 0.0005005208658985794\n",
      "Batch: 125/151, Loss: 0.0007481981301680207\n",
      "Batch: 126/151, Loss: 0.0006864832830615342\n",
      "Batch: 127/151, Loss: 0.0005850362940691411\n",
      "Batch: 128/151, Loss: 0.0007784535409882665\n",
      "Batch: 129/151, Loss: 0.0006921340827830136\n",
      "Batch: 130/151, Loss: 0.0007045367965474725\n",
      "Batch: 131/151, Loss: 0.0006637995829805732\n",
      "Batch: 132/151, Loss: 0.0004921610816381872\n",
      "Batch: 133/151, Loss: 0.0006677687051706016\n",
      "Batch: 134/151, Loss: 0.0006060648593120277\n",
      "Batch: 135/151, Loss: 0.005881969351321459\n",
      "Batch: 136/151, Loss: 0.0007430504774674773\n",
      "Batch: 137/151, Loss: 0.00046278748777695\n",
      "Batch: 138/151, Loss: 0.0008039609529078007\n",
      "Batch: 139/151, Loss: 0.0006963717169128358\n",
      "Batch: 140/151, Loss: 0.0007866232190281153\n",
      "Batch: 141/151, Loss: 0.0006706909625791013\n",
      "Batch: 142/151, Loss: 0.0007776743732392788\n",
      "Batch: 143/151, Loss: 0.000650688773021102\n",
      "Batch: 144/151, Loss: 0.0010297712869942188\n",
      "Batch: 145/151, Loss: 0.0004781292809639126\n",
      "Batch: 146/151, Loss: 0.007758801337331533\n",
      "Batch: 147/151, Loss: 0.006349792238324881\n",
      "Batch: 148/151, Loss: 0.0007067345432005823\n",
      "Batch: 149/151, Loss: 0.0005915484507568181\n",
      "Batch: 150/151, Loss: 0.0006773411296308041\n",
      "Batch: 151/151, Loss: 0.0008945692679844797\n",
      "Total Loss: 0.14950847686850466\n"
     ]
    }
   ],
   "source": [
    "# Load the model and set it to evaluation mode\n",
    "model2 = Model(encoder, decoder).to(device)\n",
    "model2.load_state_dict(torch.load(\"checkpoints/llm/epoch_10.pt\"))\n",
    "model2.eval()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "eval_dataset = TranslationDataset(\"data/llm/evaluation.en\", \"data/llm/evaluation.ko\", english_tokenizer, korean_tokenizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=True, collate_fn=padding)\n",
    "\n",
    "# Evaluate the model\n",
    "total_loss = 0\n",
    "\n",
    "for i, (english, korean) in enumerate(eval_dataloader):\n",
    "    english, korean = english.to(device), korean.to(device)\n",
    "    output = model2(english, korean)\n",
    "    output = output.view(-1, output.shape[2])\n",
    "    korean = korean.view(-1)\n",
    "    loss = criterion(output, korean)\n",
    "    total_loss += loss.item()\n",
    "    print(f\"Batch: {i+1}/{len(eval_dataloader)}, Loss: {loss.item()}\")\n",
    "print(f\"Total Loss: {total_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T06:41:53.933500Z",
     "start_time": "2025-03-14T06:41:32.810473Z"
    }
   },
   "id": "3fd7dc2370f74843"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개개개개개개개개개개개개개개개개개개개개\n"
     ]
    }
   ],
   "source": [
    "# test inference\n",
    "sentence = \"They\"\n",
    "sentence = english_tokenizer.encode(sentence)\n",
    "sentence = torch.tensor(sentence).unsqueeze(0).to(device)\n",
    "hidden, cell = model2.encoder(sentence)\n",
    "start = torch.tensor([[korean_tokenizer.bos_id()]]).to(device)\n",
    "output = []\n",
    "for _ in range(20):\n",
    "    prediction, hidden, cell = model2.decoder(start, hidden, cell)\n",
    "    prediction = prediction.argmax(2)\n",
    "    output.append(prediction.item())\n",
    "    start = prediction\n",
    "    if prediction.item() == korean_tokenizer.eos_id():\n",
    "        break\n",
    "output = [reverse_korean_vocab[i] for i in output]\n",
    "print(\"\".join(output))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T13:01:25.244424Z",
     "start_time": "2025-03-14T13:01:25.224978Z"
    }
   },
   "id": "66f147219e0b9cfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trying to debug why our results are so bad"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79c3a7dd3f8f9b90"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293]\n",
      "they\n",
      "[1590]\n",
      "They\n",
      "tensor([[1329,   13,  637, 1938,   34,   50,   40,  354,   47,   34,  351, 1936,\n",
      "          141,  392,    8, 1442,   49,   22,   11,  537,   38, 1150,  285, 1495,\n",
      "           72,    8,    4,  436,   58,   14,  553,  616,  769, 1445,  175,   25,\n",
      "          268, 1952,   38,  846,   22,  491, 1952,  176,   47,  161,   90, 1476,\n",
      "           90,   83,   11,  296, 1938, 1954],\n",
      "        [  63, 1961, 1945,   90, 1263,   22,   26,  579,  955,   75, 1394,   29,\n",
      "          418,    6,  257,  345,   72,    6,  922, 1959, 1942,  354,    4,  436,\n",
      "         1070,  263, 1954,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3]])\n",
      "▁What▁needs▁to▁be▁done▁is▁to▁scoop▁up▁the▁boiled▁soup▁and▁side▁dishes▁on▁the▁table▁that▁wife▁made▁before▁she▁goes▁out,▁and▁feed▁them,▁which▁is▁not▁as▁easy▁as▁it▁sounds.tensor([[  29,  832,  763,  180,  899,  141,   16,  757, 1760,  835, 1143,  774,\n",
      "           44,  793,  108, 1217,  764,  501,  976,  762,  632,  773,  757, 1295,\n",
      "          758,  771,  808,   58,  811,  187,  344,   83,  445,  757, 1406,  900,\n",
      "           40,  758,   29,  105,  761],\n",
      "        [ 208,  989,  420,  114,  880,  834,  757, 1517,  796,  762,  757,    2,\n",
      "          763, 1137,  808,  144,  869,  908,   57,  488,  758,   37, 1397,  922,\n",
      "         1092,  636,  761,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3]])\n",
      "▁아내가▁외출하기▁전▁끓여놓은▁국과▁반찬을▁식판에▁떠서▁먹이기만▁하면▁되는데▁말처럼▁쉬운▁일이▁아니다.[849]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[90]\u001B[39m\u001B[32m, line 42\u001B[39m\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m prediction.item() == korean_tokenizer.eos_id():\n\u001B[32m     41\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m reverse_korean_vocab[output] == \u001B[33m\"\u001B[39m\u001B[33m집\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mAssertionError\u001B[39m: "
     ]
    }
   ],
   "source": [
    "sentence = \"they\"\n",
    "print(str(english_tokenizer.encode(sentence)))\n",
    "print(english_tokenizer.decode(english_tokenizer.encode(sentence)))\n",
    "sentence = \"They\"\n",
    "print(str(english_tokenizer.encode(sentence)))\n",
    "print(english_tokenizer.decode(english_tokenizer.encode(sentence)))\n",
    "\n",
    "# setting hidden to 0 doesn't change anything\n",
    "# TODO: Change the padding token in the padding class to be end of sentence?\n",
    "# TODO: More data?\n",
    "\n",
    "# make sure that the target loader works\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=padding)\n",
    "for english, korean in loader:\n",
    "    print(english)\n",
    "    for i in english[0]:\n",
    "        print(reverse_english_vocab[i.item()], end=\"\")\n",
    "    print(korean)\n",
    "    for i in korean[0]:\n",
    "        print(reverse_korean_vocab[i.item()], end=\"\")\n",
    "    break\n",
    "    \n",
    "# Maybe checkpoint 1 is better because of overfitting?\n",
    "model3 = Model(encoder, decoder).to(device)\n",
    "model3.load_state_dict(torch.load(\"checkpoints/llm/epoch_1.pt\"))\n",
    "model3.eval()\n",
    "sentence = \"home\"\n",
    "print(str(english_tokenizer.encode(sentence)))\n",
    "assert str(english_tokenizer.encode(sentence)) != str(3)\n",
    "sentence = torch.tensor(english_tokenizer.encode(sentence)).unsqueeze(0).to(device)\n",
    "hidden, cell = model3.encoder(sentence)\n",
    "start = torch.tensor([[korean_tokenizer.bos_id()]]).to(device)\n",
    "output = None\n",
    "for _ in range(1):\n",
    "    prediction, hidden, cell = model3.decoder(start, hidden, cell)\n",
    "    prediction = prediction.argmax(2)\n",
    "    output = prediction.item() \n",
    "    start = prediction\n",
    "    if prediction.item() == korean_tokenizer.eos_id():\n",
    "        break\n",
    "assert reverse_korean_vocab[output] == \"집\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T13:17:09.300719Z",
     "start_time": "2025-03-14T13:17:09.264232Z"
    }
   },
   "id": "3e25138df7646631"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "89b6266bb2ae31d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
